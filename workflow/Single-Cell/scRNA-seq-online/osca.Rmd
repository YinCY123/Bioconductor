---
title: "Orchestrating Single-Cell Analysis with Bioconductor"
author: "yincy"
date: "2/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Chapter 1 Introduction  
```{r}
library(SingleCellExperiment)
```

### Workflows  
All workflows begin with data import and subsequent *quality control and normalization*, going from a raw (count) expression matrix to a clean one. This includes adjusting for experimental factors and possibly even latent factors. Using the clean expression matrix, *feature selection* strategies can be applied to select the features (genes) driving heterogeneity. Furthermore, these features can then be used to perform *dimensionality reduction*, which enables downstream analysis that would not otherwise be possible and visualization in 2 or 3 dimensions.  

From there, the workflows largely focus on differing downstream analyses. *Clustering* details how to segment a scRNA-seq dataset, and *differential expression* provides a means to determine what drives the differences between different groups of cells. *Integrating datasets* walks through merging scRNA-seq datasets, an area of need as the number of scRNA-seq datasets continues to grow and comparisons between datasets must be done. Finally, we touch upon how to work with *large scale data*, specifically where it becomes impractical or impossible to work with data solely in-memory.  


# Chapter 2 Learning R and Bioconductor  
- Codecademy [Learn R Series](https://www.codecademy.com/learn/learn-r)
- [R for Data Science](https://r4ds.had.co.nz/)book.  
- [tidyverse](https://www.tidyverse.org/) ecosystem  
- [Bioconductor Courses](https://bioconductor.org/help/course-materials/)  


# Chapter 3 Beyond R Basics  
## Becoming an R Expert  
- [Advanced R](https://adv-r.hadley.nz/)  
- [programming with S4](https://adv-r.hadley.nz/s4.html)  
- [R packages](http://r-pkgs.had.co.nz/)  
- [What They Forgot to Teach You About R](https://whattheyforgot.org/)  
- [R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf)  


## Nice Companions for R  
While not essential for our purposes, many bioinformatic tools for processing raw sequencing data require knowledge beyond just R to install, run, and import their results into R for further analysis. The most important of which are basic knowledge of the Shell/Bash utilities, for working with bioinformatic pipelines and troubleshooting (R package) installation issues.  

Additionally, for working with packages or software that are still in development and not hosted on an official repository like CRAN or Bioconductor, knowledge of Git - a version control system - and the popular GitHub hosting service is helpful. This enables you to not only work with other people’s code, but also better manage your own code to keep track of changes.  


### Shell/Bash  
[Shell/Bash](https://www.datacamp.com/courses/tech:shell)  

### Git
[Git](https://www.datacamp.com/courses/tech:git)  

### Other Languages  
- Python  
- C++ (Rcpp)  

# Chapter 4 Data Infrastructure  
## Background  
One of the main strengths of the Bioconductor project lies in the use of a common data infrastructure that powers interoperability across packages. Users should be able to analyze their data using functions from different Bioconductor packages without the need to convert between formats.  

```{r, fig.align='center', fig.cap='Overview of the structure of the 'SingleCellExperiment' class. Each row of the assyas corresponds to a row of the 'rowData'(pink shading), while each column of the assyas corresponds to a column of the 'colData' and 'reducedDims'(yellow shading).'}
knitr::include_graphics("figures/SingleCellExperiment.png")
```

Each piece of (meta)data in the `SingeCellExperiment` is represented by a separate 'slot'. (This terminology comes from the [S4 class system](https://adv-r.hadley.nz/s4.html)).  

## Storing primary experimental data  
### Filling the `assays` slot  
To construct a rudimentary `SingleCellExperiment` object, we only need to fill the `assays` slot. This contains primary data such as a matrix of sequencing counts where rows correspond to features (genes) and columns correspond to samples (cells).  

```{r}
counts_matrix <- data.frame(
    cell_1 = rpois(10, 10), 
    cell_2 = rpois(10, 10), 
    cell_3 = rpois(10, 30)
)

rownames(counts_matrix) <- paste("gene_", 1:10, sep = "")

counts_matrix <- as.matrix(counts_matrix) # must be a matrix object
```

```{r}
sce <- SingleCellExperiment(assays = list(counts = counts_matrix))
```

```{r}
sce
```

To access the count data  

- `assay(sce, "counts")`  
- `counts(sce)`  

```{r}
counts(sce)
```

```{r}
assays(sce, "counts")[[1]]
```

### Adding more `assays`  
What makes the `assays` slot especially powerful is that it can hold multiple representations of the primary data. This is especially useful for storing the raw count matrix as well as a normalized version of the data. We can do just that as shown below, using the `scater` package to compute a normalized and log-transformed representation of the initial primary data.  

```{r}
sce <- scater::logNormCounts(x = sce)
sce
```

```{r}
logcounts(sce)
```

```{r}
assays(sce)
```

```{r}
counts_100 <- counts(sce) + 100
assay(sce, "counts_100") <- counts_100 # assign a new entry to assays slot
assays(sce)
```


## Handling metadata  
### On the columns  
To further annotate our `SingleCellExperiment` object, we can add metadata to describe the columns of our primary data, e.g., the samples or cells of our experiment. This data is entered into the `colData` slot, a `data.frame` or `DataFrame` object where rows correspond to cells and columns correspond to metadata fields, e.g., batch of origin, treatment condition.  

```{r}
cell_metadata <- data.frame(batch = c(1, 1, 2))
rownames(cell_metadata) <- paste("cell_", 1:3, sep = "")
```

```{r}
sce <- SingleCellExperiment(assays = list(counts = counts_matrix), 
                            colData = cell_metadata)
sce
```

Access the column data with `colData()`  

```{r}
colData(sce)
```

Access the column data with `$`  

```{r}
sce$batch
```

Some functions automatically add column metadata by returning a `SingleCellExperiment` with extra fields in the `colData` slot. For example, the `scater` package contains the `addPerCellQC()` function that appends a lot of quality control data.  

```{r}
sce <- scater::addPerCellQC(x = sce)
colData(sce)
```

manually add more fields to the column metadata  

```{r}
sce$more_stuff <- runif(ncol(sce))
colnames(colData(sce))
```

A common operation with colData is to use its values for subsetting.  

```{r}
sce[, sce$batch == 1]
```

### On the rows  
To store feature-level annotation, the `SingleCellExperiment` has the `rowData` slot containing a `DataFrame` where each row corresponds to a gene and contains annotations like the transcript length or gene symbol. Furthermore, there is a special `rowRanges` slot to hold genomic coordinates in the form of a `GRanges` or `GRangesList`. This stores describes the chromosome, start, and end coordinates of the features (genes, genomic regions) in a manner that is easy to query and manipulate via the `GenomicRanges` framework.  

Both of the slots can be accessed via their respective accessors, `rowRanges()` and `rowData()`.  

```{r}
rowRanges(sce) # empty
```

```{r}
sce <- scater::addPerFeatureQC(sce)
rowData(sce)
```

The feature data could be provided at the onset when creating the `SingleCellExperiment` object.  

```{r}
library(EnsDb.Hsapiens.v86)
edb <- genes(EnsDb.Hsapiens.v86)

edb[, 2]
```

To subset a `SingleCellExperiment` object at the feature/gene level, we can do a row subsetting operation similar to other R objects, by supplying either **numeric indices or a vector of names**.  

```{r}
sce[c("gene_1", "gene_4"), ]
```

```{r}
sce[c(1, 4), ]
```


### Other metadata  
Some analyses contain results or annotations that do not fit into the aforementioned slots, e.g., study metadata. Thankfully, there is a slot just for this type of messy data - the `metadata` slot, a named list of entries where each entry in the list can be anything you want it to be.  

```{r}
my_genes <- c("gene_1", "gene_5")
metadata(sce) <- list(favorite_genes = my_genes)
metadata(sce)
```

append more information via `$`  
```{r}
your_genes <- c("gene_4", "gene_8")
metadata(sce)$your_genes <- your_genes

metadata(sce)
```


## Single-cell Specific fields  
### Background  
So far, we have covered the `assays` (primary data), `colData` (cell metadata), `rowData`/`rowRanges` (feature metadata), and `metadata` slots (other) of the `SingleCellExperiment` class. These slots are actually inherited from the `SummarizedExperiment` parent class (see here for details), so any method that works on a `SummarizedExperiment` will also work on a `SingleCellExperiment` object.  

### Dimensionality reduction results  
The `reducedDims` slot is specifically designed to store reduced dimensionality representations of the primary data obtained by methods such as PCA and t-SNE. This slot contains a list of numeric matrices of low-reduced representations of the primary data, where the rows represent the columns of the primary data, and columns represent the dimentions. As this slot holds a list, we can store multiple PCA/t-SNE/etc. results for the same dataset.  

```{r}
sce <- scater::logNormCounts(sce)
sce <- scater::runPCA(sce)
reducedDim(sce, "PCA")
```

```{r}
sce <- scater::runTSNE(sce, perplexity = 0.1)
reducedDim(sce, "TSNE")
```

```{r}
reducedDims(sce)
```

manually add content to the `reducedDims()` slot.  

```{r}
u <- uwot::umap(t(logcounts(sce)), n_neighbors = 2)

reducedDim(sce, "UMAP_uwot") <- u
reducedDims(sce)
```

```{r}
reducedDim(sce, "UMAP_uwot")
```

### Alternative Experiments  
The `SingleCellExperiment` class provides the concept of “alternative Experiments” where we have data for a distinct set of features but the same set of samples/cells. The classic application would be to store the per-cell counts for spike-in transcripts; this allows us to retain this data for downstream use but separate it from the `assays` holding the counts for endogenous genes. The separation is particularly important as such alternative features often need to be processed separately.  

If we have data for alternative feature sets, we can store it in our SingleCellExperiment as an alternative Experiment.  

```{r}
spike_counts <- cbind(cell_1 = rpois(5, 10), 
                      cell_2 = rpois(5, 10), 
                      cell_3 = rpois(5, 30))

rownames(spike_counts) <- paste("spike_", 1:5)
spike_se <- SummarizedExperiment(assays = list(counts = spike_counts))
spike_se
```

Then store this `SummarizedExperiment` in our `sce` object via the `altExp()` setter.  

```{r}
altExp(sce, "spike") <- spike_se
altExp(sce)
```

The alternative Experiment concept ensures that all relevant aspects of a single-cell dataset can be held in a single object. It is also convenient as it ensures that our spike-in data is synchronized with the data for the endogenous genes. For example, if we subsetted `sce`, the spike-in data would be subsetted to match.  

```{r}
sub <- sce[, 1:2]
altExp(sub, "spike")
```

Any `SummarizedExperiment` object can be stored as an alternative Experiment, including another `SingleCellExperiment`! This allows power users to perform tricks.   

### Size factors  
The `sizeFactors()` function allows us to get or set a numeric vector of per-cell scaling factors used for normalization. This is typically automatically added by normalization functions.  

```{r}
sce <- scran::computeSumFactors(sce)
sizeFactors(sce)
```

manually add the size factors  

```{r}
sizeFactors(sce) <- scater::librarySizeFactors(sce)
sizeFactors(sce)
```

Technically speaking, the `sizeFactors` concept is not unique to single-cell analyses. Nonetheless, we mention it here as it is an extension beyond what is available in the `SummarizedExperiment` parent class.  


### Column labels  
The `colLabels()` function allows us to get or set a vector or factor of per-cell labels, typically corresponding to groupings assigned by unsupervised clustering (see Chapter 10) or predicted cell type identities from classification algorithms.  

```{r no-this-function, eval=FALSE}
colLabels <- LETTERS[1:3]
colLabels(sce)
```


## Conclusion  
The widespread use of the `SingleCellExperiment` class provides the foundation for interoperability between single-cell-related packages in the Bioconductor ecosystem. `SingleCellExperiment` objects generated by one package can be used as input into another package, encouraging synergies that enable our analysis to be greater than the sum of its parts. Each step of the analysis will also add new entries to the `assays`, `colData`, `reducedDims`, etc., meaning that the final `SingleCellExperiment` object effectively serves as a self-contained record of the analysis. This is convenient as the object can be saved for future use or transferred to collaborators for further analysis.  


# Chapter 5 Overview  
## Introduction  
This chapter provides an overview of the framework of a typical scRNA-seq analysis workflow. Subsequent chapters will describe each analysis in more detail.  

```{r, fig.cap='Schenatic of a typical scRNA-seq analysis workflow. Each stage (separated by dashed lines) consists of a number of specific steps, many of which operate on and modify a `SingleCellExperiment` instance.'}
knitr::include_graphics("figures/scRNA-seq-analysis-workflow.png")
```


## Experimental Design  
Before starting the analysis itself, some comments on experimental design may be helpful. The most obvious question is the choice of technology, which can be roughly divided into:  
- Droplet-based: 10X Genomics, inDrop, Drop-seq  
- Plate-based with unique molecular identifiers (UMIs): CEL-seq, MARS-seq  
- Plate-based with reads: Smart-seq2  
- Other: sci-RNA-seq, Seq-Well  

Papers describe the advantages and weaknesses
- PMID: 32518403  
- PMID: 28212749  

In practical terms, <a style='color="red"'>droplet-based technologies are the current de facto standard due to their throughput and low cost per cell. Plate-based methods can capture other phenotypic information (e.g., morphology) and are more amenable to customization. Read-based methods provide whole-transcript coverage, which is useful in some applications (e.g., splicing, exome mutations); otherwise, UMI-based methods are more popular as they mitigate the effects of PCR amplification noise</a>.  

The next question is how many cells should be captured, and to what depth they should be sequenced. The short answer is “as much as you can afford to spend”. The long answer is that it depends on the aim of the analysis. If we are aiming to discover rare cell subpopulations, then we need more cells. If we are aiming to characterize subtle differences, then we need more sequencing depth. As of time of writing, an informal survey of the literature suggests that typical droplet-based experiments would capture anywhere from 10,000 to 100,000 cells, sequenced at anywhere from 1,000 to 10,000 UMIs per cell (usually in inverse proportion to the number of cells). Droplet-based methods also have a trade-off between throughput and doublet rate that affects the true efficiency of sequencing.  

For studies involving multiple samples or conditions, the design considerations are the same as those for bulk RNA-seq experiments. There should be multiple biological replicates for each condition and conditions should not be confounded with batch. Note that individual cells are not replicates; rather, we are referring to samples derived from replicate donors or cultures.  

## Obtaining a count matrix  
Sequencing data from scRNA-seq experiments must be converted into a matrix of expression values that can be used for statistical analysis. Given the discrete nature of sequencing data, this is usually a count matrix containing the number of UMIs or reads mapped to each gene in each cell. The exact procedure for quantifying expression tends to be technology-dependent:  

- For 10X Genomics data, the `CellRanger` software suite provides a custom pipeline to obtain a count matrix. This uses STAR to align reads to the reference genome and then counts the number of unique UMIs mapped to each gene.  

- Pseudo-alignment methods such as `alevin` can be used to obtain a count matrix from the same data with greater efficiency. This avoids the need for explicit alignment, which reduces the compute time and memory usage.  

- For other highly mutiplexed protocols, the `scPipe` package provides a more general pipeline for processing scRNA-seq data. This uses the `Rsubread` aligner to align reads and then counts UMIs per gene.  

- For CEL-seq or CEL-seq2 data, the `scruff` package provides a dedicated pipline for quantification.  

- For read-based protocols, we can generally re-use the same pipelines for processing bulk RNA-seq data.  

- For any data involving spike-in transcripts, the spike-in sequences should be included as part of the reference genome during alignment and quantification.  

After quantification, we import the count matrix into R and create a `SingleCellExperiment` object. This can be done with base methods (e.g., `read.table()`) followed by applying the `SingleCellExperiment()` constructor. Alternatively, for specific file formats, we can use dedicated methods from the `DropletUtils` (for 10X data) or `tximport`/`tximeta` packages (for pseudo-alignment methods). Depending on the origin of the data, this requires some vigilance:  

- Some feature-counting tools will report mapping statistics in the count matrix (e.g., the number of unaligned or unassigned reads). While these values can be useful for quality control, they would be misleading if treated as gene expression values. Thus should be removed (or at least moved to the `colData`) prior to feature analyses.  

- Be careful of using the `^RECC` regular expression to detect spike-in rows in human data where the row names of the count matrix are gene symbols. An ERCC gene family actually exists in human annotation, so this would result in incorrect identification of genes as spike-in transcripts. This problem can be avoided by using count matrices with standard identifiers (e.g., Ensembl, Entrez).  

## Data Processing and downstream analysis  
In the simplest case, the workflow has the following form:  

1. **We compute quality control metrics to remove low-quality cells that would interfere with downstream analyses**. These cells may have been damaged during processing or may not have been fully captured by the sequencing protocol. Common metrics includes the total counts per cell, the proportion of spike-in or mitochondrial reads and the number of detected features.  

2. **We convert the counts into normalized expression values to elimiinate cell-specific biases** (e.g., in capture efficiency). This allows us to perform explicit comparisons across cells in downstream steps like clustering. **We also apply a transformation, typically log, to adjust for the mean-variance relationship**.  

3. **We perform feature selection to pick a subset of interesting features for downstream analysis**. This is done by modelling the variance across cells for each gene and retaining genes that are highly variable. The aim is to reduce computational overhead and noise from uninteresting genes.   

4. **We apply dimensionality reduction to compact the data and further reduce noise**. Principal components analysis is typically used to obtain an initial low-rank representation for more computational work, followed by more aggressive methods like t-stochastic neighbor embedding for visualization purposes.  

5. **We cluster cells into groups according to similarities in their (normalized) expression profiles**. This aims to obtain groupings that serve as empirical proxies for distinct biological states. We typically interpret these groupings by identifying differentially expressed marker genes between clusters.  


## Quick start  
Here, we use the a droplet-based retina dataset from Macosko et al.([2015](http://dx.doi.org/10.1016/j.cell.2015.05.002)), provided in the scRNAseq package. This starts from a count matrix and finishes with clusters in preparation for biological interpretation. Similar workflows are available in abbreviated from the Workflows.  

```{r}
library(scRNAseq)
sce <- MacoskoRetinaData()

# Quality control
library(scater)
is.mito <- grepl("^MT-", rownames(sce))
qcstats <- perCellQCMetrics(sce, subsets = list(Mito = is.mito))
filtered <- quickPerCellQC(df = qcstats, percent_subsets = "subsets_Mito_percent")
sce <- sce[, !filtered$discard]

# Normalization  
sce <- logNormCounts(sce)

# feature selection  
library(scran)
dec <- modelGeneVar(sce)
hvg <- getTopHVGs(dec, prop = 0.1)

# dimensionality reduction
set.seed(1234)
sce <- runPCA(sce, ncomponents = 25, subset_row = hvg)
sce <- runUMAP(sce, dimred = "PCA", external_neighbors = TRUE)


# clustering
g <- buildSNNGraph(sce, use.dimred = "PCA")
colLabels(sce) <- factor(igraph::cluster_louvain(g)$membership)

# visualization
plotUMAP(sce, colour_by = "label")
```


# Chapter 6 Quality Control  
Low-quality libraries in scRNA-seq data can arise from a variety of sources such as cell damage during dissociation or failure in library preparation (e.g., inefficient reverse transcription or PCR amplification). **These usually manifest as “cells” with low total counts, few expressed genes and high mitochondrial or spike-in proportions**. These low-quality libraries are problematic as they can contribute to misleading results in downstream analyses:  

- They form their own distinct cluster(s), complicating interpretation of the results. This is most obviously driven by increased mitochondrial proportions or enrichment for nuclear RNAs after cell damage. In the worst case, low-quality libraries generated from different cell types can cluster together based on similarities in the damage-induced expression profiles, creating artificial intermediate states or trajectories between otherwise distinct subpopulations. Additionally, very small libraries can form their own clusters due to shifts in the mean upon transformation.  

- They distort the characterization of population heterogeneity during variance estimation or principal components analysis. The first few principal components will capture differences in quality rather than biology, reducing the effectiveness of dimensionality reduction. Similarly, genes with the largest variances will be driven by differences between low- and high-quality cells. The most obvious example involves low-quality libraries with very low counts where scaling normalization inflates the apparent variance of genes that happen to have a non-zero count in those libraries.  

- They contain genes that appear to be strongly “upregulated” due to aggressive scaling to normalize for small library sizes. This is most problematic for contaminating transcripts (e.g., from the ambient solution) that are present in all libraries at low but constant levels. Increased scaling in low-quality libraries transforms small counts for these transcripts in large normalized expression values, resulting in apparent upregulation compared to other cells. This can be misleading as the affected genes are often biologically sensible but are actually expressed in another subpopulation.  

To avoid - or at least mitigate - these problems, we need to remove these cells at the start of the analysis. This step is commonly referred to as quality control (QC) on the cells.  

scRNA-seq dataset from [PMID:29030468]
```{r}
library(scRNAseq)

load("data/sce.416b.RData")
sce.416b$block <- factor(sce.416b$block)
```


## Choice of QC metrics  
We use several common QC metrics to identify low-quality cells based on their expression profiles. These metrics are described below in terms of reads for SMART-seq2 data, but the same definitions apply to UMI data generated by other technologies like MARS-seq and droplet-based protocols.  

- The library size is defined as the total sum of counts across all relevant features for each cell. Here, we will consider the relevant features to be the endogenous genes. Cells with small library sizes are of low quality as the RNA has been lost at some point during library preparation, either due to cell lysis or inefficient cDNA capture and amplification.  

- The number of expressed features in each cell is defined as the number of endogenous genes with non-zero counts for that cell. Any cell with very few expressed genes is likely to be of poor quality as the diverse transcript population has not been successfully captured.  

- The proportion of reads mapped to spike-in transcripts is calculated relative to the total count across all features (including spike-ins) for each cell. As the same amount of spike-in RNA should have been added to each cell, any enrichment in spike-in counts is symptomatic of loss of endogenous RNA. Thus, high proportions are indicative of poor-quality cells where endogenous RNA has been lost due to, e.g., partial cell lysis or RNA degradation during dissociation.  

- In the absence of spike-in transcripts, the proportion of reads mapped to genes in the mitochondrial genome can be used. High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), presumably because of loss of cytoplasmic RNA from perforated cells. The reasoning is that, in the presence of modest damage, the holes in the cell membrane permit efflux of individual transcript molecules but are too small to allow mitochondria to escape, leading to a relative enrichment of mitochondrial transcripts. For single-nuclei RNA-seq experiments, high proportions are also useful as they can mark cells where the cytoplasm has not been successfully stripped.  

For each cell, we calculate these QC metrics using the `perCellQCMetrics()` function from the `scater` package (McCarthy et al. 2017). The `sum` column contains the total count for each cell and the `detected` column contains the number of detected genes. The `subsets_Mito_percent` column contains the percentage of reads mapped to mitochondrial transcripts. (For demonstration purposes, we show two different approaches of determining the genomic location of each transcript.) Finally, the `altexps_ERCC_percent` column contains the percentage of reads mapped to ERCC transcripts.  

```{r}
library(AnnotationHub)
ens.mm.v97 <- AnnotationHub()[["AH73905"]]

# retrieving the mitochondrial transcripts using genomic locations included in the low-level annotation for the SingleCellExperiment.  
location <- rownames(sce.416b) %>% 
    select(ens.mm.v97, 
           keys = ., 
           keytype = "GENEID", 
           columns = c("GENEID", "SYMBOL"))

is.mito <- grepl("^MT-", location$SYMBOL, ignore.case = T)


# # ALTERNATIVELY: using resources in AnnotationHub to retrieve chromosomal locations given the Ensembl IDs; this should yield the same result.  
# library(EnsDb.Mmusculus.v79)

# chr.loc <- select(EnsDb.Mmusculus.v79, 
#                   keys = rownames(sce.416b), 
#                   keytype = "GENEID", 
#                   column = c("SYMBOL", "GENEID"))
# chr.loc %>% head()
# 
# is.mito.alt <- grepl("^MT-", chr.loc$SYMBOL, ignore.case = T)
# is.mito.alt %>% table

library(scater)
df <- perCellQCMetrics(sce.416b, subset = list(Mito = is.mito))
```

Alternatively, users may prefer to use the addPerCellQC() function. This computes and appends the per-cell QC statistics to the colData of the SingleCellExperiment object, allowing us to retain all relevant information in a single object for later manipulation.  

```{r}
sce.416b <- addPerCellQC(x = sce.416b, subsets = list(Mito = is.mito))
colData(sce.416b) %>% colnames()
```

**A key assumption here is that the QC metrics are independent of the biological state of each cell**. Poor values (e.g., low library sizes, high mitochondrial proportions) are presumed to be driven by technical factors rather than biological processes, meaning that the subsequent removal of cells will not misrepresent the biology in downstream analyses. Major violations of this assumption would potentially result in the loss of cell types that have, say, systematically low RNA content or high numbers of mitochondria.  


## Identifying low-quality cells  
### With fixed thresholds  
The simplest approach to identifying low-quality cells is to apply thresholds on the QC metrics. For example, we might consider cells to be low quality if they have library sizes below 100,000 reads; express fewer than 5,000 genes; have spike-in proportions above 10%; or have mitochondrial proportions above 10%.  

```{r}
qc.lib <- df$sum < 1e5
qc.nexprs <- df$detected < 5e3
qc.spike <- df$altexps_ERCC_percent > 10
qc.mito <- df$subsets_Mito_percent > 0.1
discard <- qc.lib | qc.nexprs | qc.spike | qc.mito

# Summarize the number of cells removed for each reason
tibble(LibSize = sum(qc.lib), 
          NExpers = sum(qc.nexprs), 
          SpikeProp = sum(qc.spike), 
          MitoProp = sum(qc.mito), 
          Total = sum(discard))

# on the SingleCellExperiment object
# qc.sc.lib <- sce.416b$sum < 1e5
# qc.sc.nexpers <- sce.416b$detected < 5e3
# qc.sc.spike <- sce.416b$altexps_ERCC_percent > 10
# qc.sc.mito <- sce.416b$subsets_Mito_percent > 0.1
# discard <- qc.sc.lib | qc.sc.nexpers | qc.sc.spike | qc.sc.mito
# 
# sce.416b <- sce.416b[, discard]
```

While simple, this strategy requires considerable experience to determine appropriate thresholds for each experimental protocol and biological system. **Thresholds for read count-based data are simply not applicable for UMI-based data, and vice versa**. Differences in mitochondrial activity or total RNA content require constant adjustment of the mitochondrial and spike-in thresholds, respectively, for different biological systems. Indeed, even with the same protocol and system, the appropriate threshold can vary from run to run due to the vagaries of cDNA capture efficiency and sequencing depth per cell.  


### With adaptive thresholds  
#### Identifying outliers  
**To obtain an adaptive threshold, we assume that most of the dataset consists of high-quality cells. We then identify cells that are outliers for the various QC metrics, based on the median absolute deviation (MAD) from the median value of each metric across all cells**. Specifically, a value is considered an outlier if it is more than 3 MADs from the median in the “problematic” direction. This is loosely motivated by the fact that such a filter will retain 99% of non-outlier values that follow a normal distribution.  

For the 416B data, we identify cells with log-transformed library sizes that are more than 3 MADs below the median. A log-transformation is used to improve resolution at small values when type="lower". Specifically, it guarantees that the threshold is not a negative value, which would be meaningless for a non-negative metric. Furthermore, it is not uncommon for the distribution of library sizes to exhibit a heavy right tail; the log-transformation avoids inflation of the MAD in a manner that might compromise outlier detection on the left tail. (More generally, it makes the distribution seem more normal to justify the 99% rationale mentioned above.)  

```{r}
qc.lib2 <- isOutlier(metric = df$sum, log = T, type = "lower")
```

We do the same for the log-transformed number of expressed genes.  

```{r}
qc.nexpers2 <- isOutlier(df$detected, log = T, type = "lower")
```

`is.Outlier()` will also return the exact filter thresholds for each metric in the attributes of the output vector. These are useful for checking whether the automatically selected thresholds are appropriate.  

```{r}
attr(qc.lib2, "thresholds")
```

```{r}
attr(qc.nexpers2, "thresholds")
```

We identify outliers for the proportion-based metrics with the same function. These distributions frequently exhibit a heavy right tail, but unlike the two previous metrics, it is the right tail itself that contains the putative low-quality cells. Thus, we do not perform any transformation to shrink the tail - rather, our hope is that the cells in the tail are identified as large outliers. (While it is theoretically possible to obtain a meaningless threshold above 100%, this is rare enough to not be of practical concern.)  

```{r}
qc.spike2 <- isOutlier(df$altexps_ERCC_percent, type = "higher")
attr(qc.spike2, "thresholds")
```

```{r}
qc.mito2 <- isOutlier(df$subsets_Mito_percent, type = "higher")
attr(qc.mito2, "thresholds")
```

A cell that is an outlier for any of these metrics is considered to be of low quality and discarded.  

```{r}
discard2 <- qc.lib2 | qc.nexpers2 | qc.spike2 | qc.mito2

# Summarize the number of cells removed for each reason.  
library(tibble)

tibble(
    LibSize = sum(qc.lib2), 
    NExpes = sum(qc.nexpers2), 
    SpikeProp = sum(qc.spike2), 
    MitoProp = sum(qc.mito2), 
    Total = sum(discard2)
)
```

Alternatively, this entire process can be done in a single step using the `quickPerCellQC()` function. This is a wrapper that simply calls `isOutlier()` with the settings described above.  

```{r}
reasons <- quickPerCellQC(df, 
                          lib_size = "sum", 
                          n_features = "detected", 
                          percent_subsets = c("subsets_Mito_percent", 
                                              "altexps_ERCC_percent", 
                                              "altexps_SIRV_percent"))

reasons %>% as_tibble() %>% colnames()
```

With this strategy, the thresholds adapt to both the location and spread of the distribution of values for a given metric. This allows the QC procedure to adjust to changes in sequencing depth, cDNA capture efficiency, mitochondrial content, etc. without requiring any user intervention or prior experience. However, it does require some implicit assumptions that are discussed below in more detail.  

#### Assumptions of outlier detection  
Outlier detection assumes that most cells are of acceptable quality. This is usually reasonable and can be experimentally supported in some situations by visually checking that the cells are intact, e.g., on the microwell plate. If most cells are of (unacceptably) low quality, the adaptive thresholds will obviously fail as they cannot remove the majority of cells. Of course, what is acceptable or not is in the eye of the beholder - neurons, for example, are notoriously difficult to dissociate, and we would often retain cells in a neuronal scRNA-seq dataset with QC metrics that would be unacceptable in a more amenable system like embryonic stem cells.  

Another assumption mentioned earlier is that the QC metrics are independent of the biological state of each cell. This is most likely to be violated in highly heterogeneous cell populations where some cell types naturally have, e.g., less total RNA (see Figure 3A of Germain, Sonrel, and Robinson (2020)) or more mitochondria. Such cells are more likely to be considered outliers and removed, even in the absence of any technical problems with their capture or sequencing. The use of the MAD mitigates this problem to some extent by accounting for biological variability in the QC metrics. A heterogeneous population should have higher variability in the metrics among high-quality cells, increasing the MAD and reducing the chance of incorrectly removing particular cell types (at the cost of reducing power to remove low-quality cells).  

In general, these assumptions are either reasonable or their violations have little effect on downstream conclusions. Nonetheless, it is helpful to keep them in mind when interpreting the results.  

#### Considering experimental factors  
More complex studies may involve batches of cells generated with different experimental parameters (e.g., sequencing depth). In such cases, the adaptive strategy should be applied to each batch separately. It makes little sense to compute medians and MADs from a mixture distribution containing samples from multiple batches. For example, if the sequencing coverage is lower in one batch compared to the others, it will drag down the median and inflate the MAD. This will reduce the suitability of the adaptive threshold for the other batches.  

If each batch is represented by its own `SingleCellExperiment`, the `isOutlier()` function can be directly applied to each batch as shown above. However, if cells from all batches have been merged into a single `SingleCellExperiment`, the `batch=` argument should be used to ensure that outliers are identified within each batch. This allows `isOutlier()` to accommodate systematic differences in the QC metrics across batches.  

We will again illustrate using the 416B dataset, which contains two experimental factors - plate of origin and oncogene induction status. We combine these factors together and use this in the `batch=` argument to `isOutlier()` via `quickPerCellQC()`. This results in the removal of slightly more cells as the MAD is no longer inflated by (i) systematic differences in sequencing depth between batches and (ii) differences in number of genes expressed upon oncogene induction.  

```{r}
batch <- paste0(sce.416b$phenotype, "-", sce.416b$block)
batch %>% table

batch.reasons <- quickPerCellQC(df, 
                                batch = batch, 
                                percent_subsets = c("subsets_Mito_percent", 
                                                    "altexps_ERCC_percent"))

batch.reasons %>% as_tibble() %>% colSums()
```

That said, the use of `batch=` involves the stronger assumption that most cells in each batch are of high quality. If an entire batch failed, outlier detection will not be able to act as an appropriate QC filter for that batch. For example, two batches in the Grun et al. (2016) human pancreas dataset contain a substantial proportion of putative damaged cells with higher ERCC content than the other batches. This inflates the median and MAD within those batches, resulting in a failure to remove the assumed low-quality cells. In such cases, it is better to compute a shared median and MAD from the other batches and use those estimates to obtain an appropriate filter threshold for cells in the problematic batches, as shown below.  

```{r}
sce.grun <- GrunPancreasData()
sce.grun <- addPerCellQC(sce.grun)

# first attempt with batch-specific thresholds
discard.ercc <- isOutlier(sce.grun$altexps_ERCC_percent, 
                          type = "higher", 
                          batch = sce.grun$donor)

with.blocking <- plotColData(object = sce.grun, 
                             x = "donor", 
                             y = "altexps_ERCC_percent", 
                             colour_by = I(discard.ercc)) +
    ggtitle(label = "with blocking")


# second attemp, sharing information across batches to avoid dramatically different thresholds for unusual batches.  
discard.ercc2 <- isOutlier(sce.grun$altexps_ERCC_percent, 
                           type = "higher", 
                           batch = sce.grun$donor, 
                           subset = sce.grun$donor %in% c("D17", "D2", "D7"))

without.blocking <- plotColData(object = sce.grun, 
                                x = "donor", 
                                y = "altexps_ERCC_percent", 
                                colour_by = I(discard.ercc2)) +
    ggtitle(label = "without blocking")

cowplot::plot_grid(with.blocking, without.blocking)
```

To identify problematic batches, one useful rule of thumb is to find batches with QC thresholds that are themselves outliers compared to the thresholds of other batches. The assumption here is that most batches consist of a majority of high quality cells such that the threshold value should follow some unimodal distribution across “typical” batches. If we observe a batch with an extreme threshold value, we may suspect that it contains a large number of low-quality cells that inflate the per-batch MAD. We demonstrate this process below for the Grun et al. (2016) data.  

```{r}
ercc.thresholds <- attr(discard.ercc, "thresholds")["higher", ]
names(ercc.thresholds)[isOutlier(ercc.thresholds, type = "higher")]
```

If we cannot assume that most batches contain a majority of high-quality cells, then all bets are off; we must revert to the approach of picking an arbitrary threshold value (Section 6.3.1) and hoping for the best.  

### Other approaches  
Another strategy is to identify outliers in high-dimensional space based on the QC metrics for each cell. We use methods from `robustbase` to quantify the “outlyingness” of each cells based on their QC metrics, and then use `isOutlier()` to identify low-quality cells that exhibit unusually high levels of outlyingness.  

```{r}
stats <- cbind(log10(df$sum), log10(df$detected), 
               df$subsets_Mito_percent, 
               df$altexps_ERCC_percent)

library(robustbase)
outlying <- adjOutlyingness(x = stats, only.outlyingness = TRUE)
multi.outlier <- isOutlier(outlying, type = "higher")
summary(multi.outlier)
```

This and related approaches like PCA-based outlier detection and support vector machines can provide more power to distinguish low-quality cells from high-quality counterparts (Ilicic et al. 2016) as they can exploit patterns across many QC metrics. However, this comes at some cost to interpretability, as the reason for removing a given cell may not always be obvious.  

For completeness, we note that outliers can also be identified from the gene expression profiles, rather than QC metrics. We consider this to be a risky strategy as it can remove high-quality cells in rare populations.  

## Checking diagnostic plots  
It is good practice to inspect the distributions of QC metrics (Figure 6.2) to identify possible problems. In the most ideal case, we would see normal distributions that would justify the 3 MAD threshold used in outlier detection. A large proportion of cells in another mode suggests that the QC metrics might be correlated with some biological state, potentially leading to the loss of distinct cell types during filtering; or that there were inconsistencies with library preparation for a subset of cells, a not-uncommon phenomenon in plate-based protocols. Batches with systematically poor values for any metric can then be quickly identified for further troubleshooting or outright removal, much like in (Figure 6.1) above.  

```{r}
colData(sce.416b) <- cbind(colData(sce.416b), df)
sce.416b$block <- factor(sce.416b$block)
sce.416b$phenotype <- ifelse(grepl("induced", sce.416b$phenotype), "induced", "wild type")
sce.416b$discard <- reasons$discard

cowplot::plot_grid(plotlist = list(
    plotColData(sce.416b, x = "block", 
                y = "sum", 
                colour_by = "discard", 
                other_fields = "phenotype") +
        facet_wrap(~phenotype) +
        ggtitle("Total count"), 
    plotColData(sce.416b, x = "block", 
                y = "detected", 
                colour_by = "discard", 
                other_fields = "phenotype") +
        facet_wrap(~phenotype) +
        ggtitle("Detected features"), 
    plotColData(sce.416b, 
                x = "block", 
                y = "subsets_Mito_detected.1", 
                colour_by = "discard", 
                other_fields = "phenotype") +
        facet_wrap(~phenotype) +
        ggtitle("Mito percent"), 
    plotColData(sce.416b, 
                x = "block", 
                y = "altexps_ERCC_percent", 
                other_fields = "phenotype") +
        facet_wrap(~phenotype) +
        ggtitle("ERCC percent")
), 
ncol = 1)
```

Another useful diagnostic involves plotting the proportion of mitochondrial counts against some of the other QC metrics. The aim is to confirm that there are no cells with both large total counts and large mitochondrial counts, to ensure that we are not inadvertently removing high-quality cells that happen to be highly metabolically active (e.g., hepatocytes). We demonstrate using data from a larger experiment involving the mouse brain (Zeisel et al. 2015); in this case, we do not observe any points in the top-right corner in Figure 6.3 that might potentially correspond to metabolically active, undamaged cells.  

```{r}
# sce.zeisel <- ZeiselBrainData()
sce.zeisel <- readRDS("data/sce.zeisel.rds")

library(scater)
sce.zeisel <- aggregateAcrossFeatures(x = sce.zeisel, 
                                      ids = sub("_loc[0-9]+$", "", rownames(sce.zeisel)))

library(org.Mm.eg.db)
rowData(sce.zeisel)$Ensembl <- mapIds(org.Mm.eg.db, 
                                      keys = rownames(sce.zeisel), 
                                      keytype = "SYMBOL", 
                                      column = "ENSEMBL")

sce.zeisel <- addPerCellQC(sce.zeisel, 
                           subsets = list(Mt = rowData(sce.zeisel)$featureType == "mito"))

qc <- quickPerCellQC(colData(sce.zeisel), 
                     percent_subsets = c("altexps_ERCC_percent", "subsets_Mt_percent"))
sce.zeisel$discard <- qc$discard

plotColData(sce.zeisel, 
            x = "sum", 
            y = "subsets_Mt_percent", 
            colour_by = "discard")
```

Comparison of the ERCC and mitochondrial percentages can also be informative (Figure 6.4). Low-quality cells with small mitochondrial percentages, large spike-in percentages and small library sizes are likely to be stripped nuclei, i.e., they have been so extensively damaged that they have lost all cytoplasmic content. On the other hand, **cells with high mitochondrial percentages and low ERCC percentages may represent undamaged cells that are metabolically active**. This interpretation also applies for single-nuclei studies but with a switch of focus: the stripped nuclei become the libraries of interest while the undamaged cells are considered to be low quality.  

```{r}
plotColData(sce.zeisel, 
            x = "altexps_ERCC_percent", 
            y = "subsets_Mt_percent", 
            colour_by = "discard")
```

We see that all of these metrics exhibit weak correlations to each other, presumably a manifestation of a common underlying effect of cell damage. The weakness of the correlations motivates the use of several metrics to capture different aspects of technical quality. Of course, the flipside is that these metrics may also represent different aspects of biology, increasing the risk of discarding entire cell types.  

## Removing low-quality cells  
Once low-quality cells have been identified, we can choose to either remove them or mark them. Removal is the most straightforward option and is achieved by subsetting the `SingleCellExperiment` by column. In this case, we use the low-quality calls from Section 6.3.2.3 to generate a subsetted `SingleCellExperiment` that we would use for downstream analyses.  

```{r}
# keeping the columns we don't want to discard.  

filtered <- sce.416b[, !reasons$discard]
```

The biggest practical concern during QC is whether an entire cell type is inadvertently discarded. There is always some risk of this occurring as the QC metrics are never fully independent of biological state. We can diagnose cell type loss by looking for systematic differences in gene expression between the discarded and retained cells. To demonstrate, we compute the average count across the discarded and retained pools in the 416B data set, and we compute the log-fold change between the pool averages.

```{r}
# using the `discard` vector for demonstration purpose, as it has more cells for stable calculation of 'lost'.

lost <- calculateAverage(x = counts(sce.416b)[, !reasons$discard])
kept <- calculateAverage(x = counts(sce.416b)[, reasons$discard])

library(edgeR)
logged <- cpm(cbind(lost, kept), log = T, prior.count = 2)
logFC <- logged[, 1] - logged[, 2]
abundance <- rowMeans(logged)
```

If the discarded pool is enriched for a certain cell type, we should observe increased expression of the corresponding marker genes. No systematic upregulation of genes is apparent in the discarded pool, suggesting that the QC step did not inadvertently filter out a cell type in the 416B dataset.  

```{r, fig.cap='Log-fold change in expression in the discarded cells compared to the retained cells in the 416B dataset. Each point represents a gene with mitochondrial transcripts in blue.'}
plot(abundance, logFC, xlab = "Average count", ylab = "Log-FC(lost/kept)", pch = 16)
points(abundance[is.mito], logFC[is.mito], col = "dodgerblue", pch  =16)
```

For comparison, let us consider the QC step for the PBMC dataset from 10X Genomics (Zheng et al. 2017). We’ll apply an arbitrary fixed threshold on the library size to filter cells rather than using any outlier-based method. Specifically, we remove all libraries with a library size below 500.  

```{r}
library(DropletTestFiles)
raw.path <- getTestFile("tenx-2.1.0-pbmc4k/1.0.0/raw.tar.gz")
out.path <- file.path("/home/yincy/git/Bioconductor/workflow/Single-Cell/scRNA-seq-online/data/", "pbmc4k")
untar(raw.path, exdir = out.path)


library(DropletUtils)
library(scuttle)
fname <- file.path(out.path, "raw_gene_bc_matrices/GRCh38")
# sce.pbmc <- read10xCounts(fname, col.names = T)
sce.pbmc <- readRDS("data/sce.pbmc.rds")

# gene annotation 
library(scater)

rownames(sce.pbmc) <- uniquifyFeatureNames(rowData(sce.pbmc)$ID, 
                                           rowData(sce.pbmc)$Symbol)

library(EnsDb.Hsapiens.v86)
location <- mapIds(EnsDb.Hsapiens.v86, 
                   keys = rowData(sce.pbmc)$ID, 
                   keytype = "GENEID", 
                   column = "SEQNAME")

# cell detection
set.seed(100)
e.out <- emptyDrops(counts(sce.pbmc))
sce.pbmc <- sce.pbmc[, which(e.out$FDR <= 0.001)]
```

```{r}
discard <- colSums(counts(sce.pbmc)) < 500
lost <- calculateAverage(counts(sce.pbmc)[, discard])
kept <- calculateAverage(counts(sce.pbmc)[, !discard])

logged <- edgeR::cpm(cbind(lost, kept), log = T, prior.count = 2)
logFC <- logged[, 1] - logged[, 2]
abundance <- rowMeans(logged)
```

The presence of a distinct population in the discarded pool manifests as a set of genes that are strongly upregulated in `lost`. This includes PF4, PPBP and SDPR, which (spoiler alert!) indicates that there is a platelet population that has been discarded by `alt.discard`.  

```{r, fig.cap='Average counts across all discarded and retained cells in the PBMC dataset, after using a more stringent filter on the total UMI count. Each point represents a gene, with platelet-related genes highlighted in orange.'}
plot(abundance, logFC, xlab = "Average count", ylab = "Log-FC (lost/kept)", pch = 16)
platelet <- c("PF4", "PPBP", "SDPR")
points(abundance[platelet], logFC[platelet], col = "orange", pch = 16)
```

If we suspect that cell types have been incorrectly discarded by our QC procedure, the most direct solution is to relax the QC filters for metrics that are associated with genuine biological differences. For example, outlier detection can be relaxed by increasing nmads= in the isOutlier() calls. Of course, this increases the risk of retaining more low-quality cells and encountering the problems discussed in Section 6.1. The logical endpoint of this line of reasoning is to avoid filtering altogether, as discussed in Section 6.6.  

As an aside, it is worth mentioning that the true technical quality of a cell may also be correlated with its type. (This differs from a correlation between the cell type and the QC metrics, as the latter are our imperfect proxies for quality.) This can arise if some cell types are not amenable to dissociation or microfluidics handling during the scRNA-seq protocol. In such cases, it is possible to “correctly” discard an entire cell type during QC if all of its cells are damaged. Indeed, concerns over the computational removal of cell types during QC are probably minor compared to losses in the experimental protocol.  

## Marking low-quality cells  
The other option is to simply mark the low-quality cells as such and retain them in the downstream analysis. The aim here is to allow clusters of low-quality cells to form, and then to identify and ignore such clusters during interpretation of the results. This approach avoids discarding cell types that have poor values for the QC metrics, giving users an opportunity to decide whether a cluster of such cells represents a genuine biological state.  

```{r}
colData(sce.416b) <- cbind(colData(sce.416b), reasons)
marked <- sce.416b
```

The downside is that it shifts the burden of QC to the interpretation of the clusters, which is already the bottleneck in scRNA-seq data analysis (Chapters 10, 11 and 12). Indeed, if we do not trust the QC metrics, we would have to distinguish between genuine cell types and low-quality cells based only on marker genes, and this is not always easy due to the tendency of the latter to “express” interesting genes (Section 6.1). Retention of low-quality cells also compromises the accuracy of the variance modelling, requiring, e.g., use of more PCs to offset the fact that the early PCs are driven by differences between low-quality and other cells.  

For routine analyses, we suggest performing removal by default to avoid complications from low-quality cells. This allows most of the population structure to be characterized with no - or, at least, fewer - concerns about its validity. Once the initial analysis is done, and if there are any concerns about discarded cell types (Section 6.5), a more thorough re-analysis can be performed where the low-quality cells are only marked. This recovers cell types with low RNA content, high mitochondrial proportions, etc. that only need to be interpreted insofar as they “fill the gaps” in the initial analysis.  


# Chapter 7 Normalization  
## Motivation  
Systematic differences in sequencing coverage between libraries are often observed in single-cell RNA sequencing data (Stegle, Teichmann, and Marioni 2015). **They typically arise from technical differences in cDNA capture or PCR amplification efficiency across cells, attributable to the difficulty of achieving consistent library preparation with minimal starting material**. Normalization aims to remove these differences such that they do not interfere with comparisons of the expression profiles between cells. This ensures that any observed heterogeneity or differential expression within the cell population are driven by biology and not technical biases.  

At this point, it is worth noting the difference between normalization and batch correction (Chapter 28.8). Normalization occurs regardless of the batch structure and only considers technical biases, while batch correction - as the name suggests - only occurs across batches and must consider both technical biases and biological differences. Technical biases tend to affect genes in a similar manner, or at least in a manner related to their biophysical properties (e.g., length, GC content), while biological differences between batches can be highly unpredictable. As such, these two tasks involve different assumptions and generally involve different computational methods (though some packages aim to perform both steps at once, e.g., zinbwave). Thus, it is important to avoid conflating “normalized” and “batch-corrected” data, as these usually refer to different things.  

We will mostly focus our attention on scaling normalization, which is the simplest and most commonly used class of normalization strategies. This involves dividing all counts for each cell by a cell-specific scaling factor, often called a “size factor” (Anders and Huber 2010). The assumption here is that any cell-specific bias (e.g., in capture or amplification efficiency) affects all genes equally via scaling of the expected mean count for that cell. The size factor for each cell represents the estimate of the relative bias in that cell, so division of its counts by its size factor should remove that bias. The resulting “normalized expression values” can then be used for downstream analyses such as clustering and dimensionality reduction. To demonstrate, we will use the Zeisel et al. (2015) dataset from the `scRNAseq` package.  

```{r}
library(scRNAseq)
sce.zeisel <- ZeiselBrainData()
sce.zeisel <- readRDS("data/sce.zeisel.rds")

library(scater)
sce.zeisel <- aggregateAcrossFeatures(x = sce.zeisel, 
                                      ids = sub("_loc[0-9]+$", "", rownames(sce.zeisel)))

# gene annotation  
library(org.Mm.eg.db)
rowData(sce.zeisel)$Ensembl <- mapIds(org.Mm.eg.db, 
                                      keys = rownames(sce.zeisel), 
                                      keytype = "SYMBOL", 
                                      column = "ENSEMBL")

# quality control
stats <- perCellQCMetrics(sce.zeisel, 
                          subsets = list(Mt = rowData(sce.zeisel)$featureType == "mito"))
qc <- quickPerCellQC(stats, percent_subsets = c("altexps_ERCC_percent", "subsets_Mt_percent"))
sce.zeisel <- sce.zeisel[, !qc$discard]
```

## Library size normalization  
Library size normalization is the simplest strategy for performing scaling normalization. We define the library size as the total sum of counts across all genes for each cell, the expected value of which is assumed to scale with any cell-specific biases. The “library size factor” for each cell is then directly proportional to its library size where the proportionality constant is defined such that the mean size factor across all cells is equal to 1. This definition ensures that the normalized expression values are on the same scale as the original counts, which is useful for interpretation - especially when dealing with transformed data (see Section 7.5.1).  

```{r}
library(scater)
lib.sf.zeisel <- librarySizeFactors(sce.zeisel)
summary(lib.sf.zeisel)
```

In the Zeisel brain data, the library size factors differ by up to 10-fold across cells (Figure 7.1). This is typical of the variability in coverage in scRNA-seq data.  

```{r, fig.cap='Distribution of size factors derived from the library size in the Zeisel brain dataset.'}
hist(log10(lib.sf.zeisel), xlab = "Log10[size factor]", col = "grey80", breaks = 20)
```

Strictly speaking, the use of library size factors assumes that there is no “imbalance” in the differentially expressed (DE) genes between any pair of cells. That is, any upregulation for a subset of genes is cancelled out by the same magnitude of downregulation in a different subset of genes. This ensures that the library size is an unbiased estimate of the relative cell-specific bias by avoiding composition effects (Robinson and Oshlack 2010). However, balanced DE is not generally present in scRNA-seq applications, which means that library size normalization may not yield accurate normalized expression values for downstream analyses.  

**In practice, normalization accuracy is not a major consideration for exploratory scRNA-seq data analyses. Composition biases do not usually affect the separation of clusters, only the magnitude - and to a lesser extent, direction - of the log-fold changes between clusters or cell types**. As such, library size normalization is usually sufficient in many applications where the aim is to identify clusters and the top markers that define each cluster.  

## Normalization by deconvolution  
As previously mentioned, composition biases will be present when any unbalanced differential expression exists between samples. Consider the simple example of two cells where a single gene **X** is upregulated in one cell **A** compared to the other cell **B**. This upregulation means that either (i) more sequencing resources are devoted to **X** in **A**, thus decreasing coverage of all other non-DE genes when the total library size of each cell is experimentally fixed (e.g., due to library quantification); or (ii) the library size of **A** increases when **X** is assigned more reads or UMIs, increasing the library size factor and yielding smaller normalized expression values for all non-DE genes. In both cases, the net effect is that non-DE genes in **A** will incorrectly appear to be downregulated compared to **B**.  

The removal of composition biases is a well-studied problem for bulk RNA sequencing data analysis. Normalization can be performed with the `estimateSizeFactorsFromMatrix()` function in the `DESeq2` package (Anders and Huber 2010; Love, Huber, and Anders 2014) or with the `calcNormFactors()` function (Robinson and Oshlack 2010) in the `edgeR` package. These assume that most genes are not DE between cells. Any systematic difference in count size across the non-DE majority of genes between two cells is assumed to represent bias that is used to compute an appropriate size factor for its removal.  

However, single-cell data can be problematic for these bulk normalization methods due to the dominance of low and zero counts. To overcome this, we pool counts from many cells to increase the size of the counts for accurate size factor estimation (Lun, Bach, and Marioni 2016). Pool-based size factors are then “deconvolved” into cell-based factors for normalization of each cell’s expression profile. This is performed using the `calculateSumFactors()` function from `scran`.  

```{r}
library(scran)
set.seed(100)

clust.zeisel <- quickCluster(sce.zeisel)
table(clust.zeisel)
```

```{r}
deconv.sf.zeisel <- calculateSumFactors(sce.zeisel, cluster = clust.zeisel)
summary(deconv.sf.zeisel)
```

We use a pre-clustering step with `quickCluster()` where cells in each cluster are normalized separately and the size factors are rescaled to be comparable across clusters. This avoids the assumption that most genes are non-DE across the entire population - only a non-DE majority is required between pairs of clusters, which is a weaker assumption for highly heterogeneous populations. By default, `quickCluster()` will use an approximate algorithm for PCA based on methods from the `irlba` package. The approximation relies on stochastic initialization so we need to set the random seed (via `set.seed()`) for reproducibility.  

We see that the deconvolution size factors exhibit cell type-specific deviations from the library size factors in Figure 7.2. This is consistent with the presence of composition biases that are introduced by strong differential expression between cell types. Use of the deconvolution size factors adjusts for these biases to improve normalization accuracy for downstream applications.  

```{r, fig.cap='Deconvolution size factor for each cell in the Zeisel brain dataset, compared to the equivalent size factor derived from the library size. The red line corresponds to identity between the two size factors.'}
plot(lib.sf.zeisel, deconv.sf.zeisel, 
     xlab = "library size factor", 
     ylab = "Deconvolution size factor", 
     log = "xy", 
     pch = 16, 
     col = as.integer(factor(sce.zeisel$level1class)))
abline(a = 0, b = 1, col = "red")
```

**Accurate normalization is most important for procedures that involve estimation and interpretation of per-gene statistics**. For example, composition biases can compromise DE analyses by systematically shifting the log-fold changes in one direction or another. However, it tends to provide less benefit over simple library size normalization for cell-based analyses such as clustering. The presence of composition biases already implies strong differences in expression profiles, so changing the normalization strategy is unlikely to affect the outcome of a clustering procedure.  

## Normalization by spike-in  
Spike-in normalization is based on the assumption that the same amount of spike-in RNA was added to each cell (A. T. L. Lun et al. 2017). Systematic differences in the coverage of the spike-in transcripts can only be due to cell-specific biases, e.g., in capture efficiency or sequencing depth. To remove these biases, we equalize spike-in coverage across cells by scaling with “spike-in size factors”. **Compared to the previous methods, spike-in normalization requires no assumption about the biology of the system (i.e., the absence of many DE genes). Instead, it assumes that the spike-in transcripts were (i) added at a constant level to each cell, and (ii) respond to biases in the same relative manner as endogenous genes**.  

**Practically, spike-in normalization should be used if differences in the total RNA content of individual cells are of interest and must be preserved in downstream analyses**. For a given cell, an increase in its overall amount of endogenous RNA will not increase its spike-in size factor. This ensures that the effects of total RNA content on expression across the population will not be removed upon scaling. **By comparison, the other normalization methods described above will simply interpret any change in total RNA content as part of the bias and remove it**.  

We demonstrate the use of spike-in normalization on a different dataset involving T cell activation after stimulation with T cell recepter ligands of varying affinity (Richard et al. 2018).  

```{r}
library(scRNAseq)
# sce.richard <- RichardTCellData()
# saveRDS(sce.richard)
sce.richard <- readRDS("data/sce.richard.rds")
sce.richard <- sce.richard[, sce.richard$`single cell quality` == 'OK']
```

We apply the `computeSpikeFactors()` method to estimate spike-in size factors for all cells. This is defined by converting the total spike-in count per cell into a size factor, using the same reasoning as in `librarySizeFactors()`. Scaling will subsequently remove any differences in spike-in coverage across cells.  

```{r}
sce.richard <- computeSpikeFactors(x = sce.richard, spikes = "ERCC")
summary(sizeFactors(sce.richard))
```

We observe a positive correlation between the spike-in size factors and deconvolution size factors within each treatment condition (Figure 7.3), indicating that they are capturing similar technical biases in sequencing depth and capture efficiency. However, we also observe that increasing stimulation of the T cell receptor - in terms of increasing affinity or time - results in a decrease in the spike-in factors relative to the library size factors. This is consistent with an increase in biosynthetic activity and total RNA content during stimulation, which reduces the relative spike-in coverage in each library (thereby decreasing the spike-in size factors) but increases the coverage of endogenous genes (thus increasing the library size factors).  

```{r, fig.cap='Size factors from spike-in normalization, plotted against the library size factor fro all cells in the T cell dataset. Each plot represents a different ligand treatment and each point is a cell coloured according by time from stimulation.'}
to.plot <- data.frame(
    DeconvFactor = calculateSumFactors(sce.richard), 
    SpikeFactor = sizeFactors(sce.richard), 
    Stimulus = sce.richard$stimulus, 
    Time = sce.richard$time
)

ggplot(to.plot, aes(x = DeconvFactor, y = SpikeFactor, color = Time)) +
    geom_point() +
    facet_wrap(~Stimulus) +
    scale_x_log10() +
    scale_y_log10() +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    theme(legend.position = "top", 
          strip.text = element_text(size = 7))
```

The differences between these two sets of size factors have real consequences for downstream interpretation. If the spike-in size factors were applied to the counts, the expression values in unstimulated cells would be scaled up while expression in stimulated cells would be scaled down. However, the opposite would occur if the deconvolution size factors were used. This can manifest as shifts in the magnitude and direction of DE between conditions when we switch between normalization strategies, as shown below for *Malat1*.  

```{r, fig.cap='Distribution of log-normalized expression values for Malat1 after normalization with the deconvolution (left) or spike-in size factor (right). Cells are stratified by the ligand affinity and colored by the time stimulation.'}
# see below for explanation of LogNormCounts()
sce.richard.deconv <- logNormCounts(sce.richard, size_factors = to.plot$DeconvFactor)
sce.richard.spike <- logNormCounts(sce.richard, size_factors = to.plot$SpikeFactor)

cowplot::plot_grid(plotlist = list(
    plotExpression(sce.richard.deconv, x = "stimulus", 
                   colour_by = "time", features = "ENSMUSG00000092341") +
        theme(axis.text.x = element_text(angle = 90)) +
        ggtitle("After deconvolution"), 
    plotExpression(sce.richard.spike, x = "stimulus", 
                   colour_by = "time", features = "ENSMUSG00000092341") +
        theme(axis.text.x = element_text(angle = 90)) +
        ggtitle("After spike-in normalization")
), 
ncol = 2)
```

Whether or not total RNA content is relevant – and thus, the choice of normalization strategy – depends on the biological hypothesis. In most cases, changes in total RNA content are not interesting and can be normalized out by applying the library size or deconvolution factors. However, this may not always be appropriate if differences in total RNA are associated with a biological process of interest, e.g., cell cycle activity or T cell activation. Spike-in normalization will preserve these differences such that any changes in expression between biological groups have the correct sign.  

**However**! Regardless of whether we care about total RNA content, it is critical that the spike-in transcripts are normalized using the spike-in size factors. Size factors computed from the counts for endogenous genes should not be applied to the spike-in transcripts, precisely because the former captures differences in total RNA content that are not experienced by the latter. Attempting to normalize the spike-in counts with the gene-based size factors will lead to over-normalization and incorrect quantification. Thus, if normalized spike-in data is required, we must compute a separate set of size factors for the spike-in transcripts; this is automatically performed by functions such as `modelGeneVarWithSpikes()`.

## Applying the size factors  
### Scaling and log-transforming  
Once we have computed the size factors, we use the `logNormCounts()` function from `scater` to compute normalized expression values for each cell. This is done by dividing the count for each gene/spike-in transcript with the appropriate size factor for that cell. The function also log-transforms the normalized values, creating a new assay called `"logcounts"`. (Technically, these are “log-transformed normalized expression values”, but that’s too much of a mouthful to fit into the assay name.) **These log-values will be the basis of our downstream analyses in the following chapters**.  

```{r}
set.seed(100)
clust.zeisel <- quickCluster(sce.zeisel)
sce.zeisel <- computeSumFactors(sce.zeisel, cluster = clust.zeisel, min.mean = 0.1)
sce.zeisel <- logNormCounts(sce.zeisel)
assayNames(sce.zeisel)
```

The log-transformation is useful as differences in the log-values represent log-fold changes in expression. This is important in downstream procedures based on Euclidean distances, which includes many forms of clustering and dimensionality reduction. By operating on log-transformed data, we ensure that these procedures are measuring distances between cells based on log-fold changes in expression. Or in other words, which is more interesting - a gene that is expressed at an average count of 50 in cell type A and 10 in cell type B, or a gene that is expressed at an average count of 1100 in A and 1000 in B? Log-transformation focuses on the former by promoting contributions from genes with strong relative differences.  

When log-transforming, we typically add a pseudo-count to avoid undefined values at zero. **Larger pseudo-counts will effectively shrink the log-fold changes between cells towards zero for low-abundance genes, meaning that downstream high-dimensional analyses will be driven more by differences in expression for high-abundance genes. Conversely, smaller pseudo-counts will increase the relative contribution of low-abundance genes**. Common practice is to use a pseudo-count of 1, for the simple pragmatic reason that it preserves sparsity in the original matrix (i.e., zeroes in the input remain zeroes after transformation). This works well in all but the most pathological scenarios (A. Lun 2018).  

Incidentally, the addition of the pseudo-count is the motivation for the centering of the size factors at unity. This ensures that both the pseudo-count and the normalized expression values are on the same scale; a pseudo-count of 1 can be interpreted as an extra read or UMI for each gene. In practical terms, centering means that the shrinkage effect of the pseudo-count diminishes as sequencing depth improves. This correctly ensures that estimates of the log-fold change in expression (e.g., from differences in the log-values between groups of cells) become increasingly accurate with deeper coverage. In contrast, if we applied a constant pseudo-count to some count-per-million-like measure, accuracy of the subsequent log-fold changes would never improve regardless of how much additional sequencing we performed.

### Downsampling and log-transforming  
**In rare cases, direct scaling of the counts is not appropriate due to the effect described by A. Lun (2018). Briefly, this is caused by the fact that the mean of the log-normalized counts is not the same as the log-transformed mean of the normalized counts**. The difference between them depends on the mean and variance of the original counts, such that there is a systematic trend in the mean of the log-counts with respect to the count size. This typically manifests as trajectories correlated strongly with library size even after library size normalization, as shown in Figure 7.5 for synthetic scRNA-seq data generated with a pool-and-split approach (Tian et al. 2019).  

```{r, fig.cap='PCA plot of all pool-and-split libraries in the SORT-seq CellBench data, computed from the log-normalized expression values with library size-derived size factors. Each point represents a library and is colored by the mixing ratio used to construct it (left) or by the size factor (right).'}
load("data/mRNAmix_qc.RData")
sce.8qc <- sce8_qc
rm(sce8_qc)
# library size normalization and log-transformation.
sce.8qc <- logNormCounts(sce.8qc)
sce.8qc <- runPCA(sce.8qc)

cowplot::plot_grid(plotlist = list(
    plotPCA(sce.8qc, colour_by = I(factor(sce.8qc$mix))), 
    plotPCA(sce.8qc, colour_by = I(librarySizeFactors(sce.8qc)))
), 
ncol = 2)
```

As the problem arises from differences in the sizes of the counts, the most straightforward solution is to downsample the counts of the high-coverage cells to match those of low-coverage cells. This uses the size factors to determine the amount of downsampling for each cell required to reach the 1st percentile of size factors. (The small minority of cells with smaller size factors are simply scaled up. We do not attempt to downsample to the smallest size factor, as this would result in excessive loss of information for one aberrant cell with very low size factors.) We can see that this eliminates the library size factor-associated trajectories from the first two PCs, improving resolution of the known differences based on mixing ratios (Figure 7.6). The log-transformation is still necessary but no longer introduces a shift in the means when the sizes of the counts are similar across cells.  

```{r, fig.cap='PCA plot of pool-and-split libraries in the SORT-seq CellBench data, computed from the log-transformed counts after downsampling in proportion to the library size factors. Each point represents a library and is colored by the mixing ratio used to construct it (left) or by the size factor (right).'}
sce.8qc2 <- logNormCounts(sce.8qc, downsample = TRUE)
sce.8qc2 <- runPCA(sce.8qc2)
cowplot::plot_grid(plotlist = list(
    plotPCA(sce.8qc2, colour_by = I(factor(sce.8qc2$mix))), 
    plotPCA(sce.8qc2, colour_by = I(librarySizeFactors(sce.8qc2)))
), 
ncol = 2)
```

While downsampling is an expedient solution, it is statistically inefficient as it needs to increase the noise of high-coverage cells in order to avoid differences with low-coverage cells. It is also slower than simple scaling. Thus, we would only recommend using this approach after an initial analysis with scaled counts reveals suspicious trajectories that are strongly correlated with the size factors. In such cases, it is a simple matter to re-normalize by downsampling to determine whether the trajectory is an artifact of the log-transformation.  

### Other options  
Of course, log-transformation is not the only possible transformation. More sophisticated approaches can be used such as dedicated *variance stabilizing transformations* (e.g., from the `DESeq2` or `sctransform` packages), which out-perform the log-transformation for removal of the mean-variance trend. In practice, though, the log-transformation is a good default choice due to its simplicity (a.k.a., reliability, predictability and computational efficiency) and interpretability.  


# Chapter 8 Feature selection  
## Motivation  
We often use scRNA-seq data in exploratory analyses to characterize heterogeneity across cells. Procedures like clustering and dimensionality reduction compare cells based on their gene expression profiles, which involves aggregating per-gene differences into a single (dis)similarity metric between a pair of cells. The choice of genes to use in this calculation has a major impact on the behavior of the metric and the performance of downstream methods. We want to select genes that contain useful information about the biology of the system while removing genes that contain random noise. This aims to preserve interesting biological structure without the variance that obscures that structure, and to reduce the size of the data to improve computational efficiency of later steps.  

**The simplest approach to feature selection is to select the most variable genes based on their expression across the population**. This assumes that genuine biological differences will manifest as increased variation in the affected genes, compared to other genes that are only affected by technical noise or a baseline level of “uninteresting” biological variation (e.g., from transcriptional bursting). Several methods are available to quantify the variation per gene and to select an appropriate set of highly variable genes (HVGs). We will discuss these below using the 10X PBMC dataset for demonstration.  

```{r}
library(DropletTestFiles)
library(DropletUtils)
fname <- file.path("data/pbmc4k/raw_gene_bc_matrices/GRCh38/")
sce.pbmc <- read10xCounts(fname, col.names = T)

# gene annotation  
library(scater)
rownames(sce.pbmc) <- uniquifyFeatureNames(
    ID = rowData(sce.pbmc)$ID, 
    names = rowData(sce.pbmc)$Symbol
)

library(EnsDb.Hsapiens.v86)
location <- mapIds(EnsDb.Hsapiens.v86, 
                   keys = rowData(sce.pbmc)$ID, 
                   keytype = "GENEID", 
                   column = "SEQNAME")

# cell detection
set.seed(100)
e.out <- emptyDrops(counts(sce.pbmc))
sce.pbmc <- sce.pbmc[, which(e.out$FDR <= 0.001)]

# quality-control
stats <- perCellQCMetrics(sce.pbmc, subsets = list(Mito = which(location == "MT")))
high.mito <- isOutlier(stats$subsets_Mito_percent, type = "higher")
sce.pbmc <- sce.pbmc[, !high.mito]

# normalization
library(scran)
set.seed(1000)
clusters <- quickCluster(sce.pbmc)
sce.pbmc <- logNormCounts(sce.pbmc)
```

```{r}
library(scRNAseq)

sce.416b <- readRDS("data/sce.416b.rds")
sce.416b$block <- factor(sce.416b$block)

# gene annotation 
library(AnnotationHub)
ens.mm.v97 <- AnnotationHub()[["AH73905"]]
rowData(sce.416b)$ENSEMBL <- rownames(sce.416b)
rowData(sce.416b)$SYMBOL <- mapIds(ens.mm.v97, 
                                   keys = rownames(sce.416b), 
                                   keytype = "GENEID", 
                                   column = "SYMBOL")

rowData(sce.416b)$SEQNAME <- mapIds(ens.mm.v97, 
                                    keys = rownames(sce.416b), 
                                    keytype = "GENEID", 
                                    column = "SEQNAME")

library(scater)
rownames(sce.416b) <- uniquifyFeatureNames(
    ID = rowData(sce.416b)$ENSEMBL,
    names = rowData(sce.416b)$SYMBOL
)

# quality control
mito <- which(rowData(sce.416b)$SEQNAME == "MT")
stats <- perCellQCMetrics(sce.416b, subsets = list(Mt = mito))
qc <- quickPerCellQC(stats, 
                     percent_subsets = c("subsets_Mt_percent", "altexps_ERCC_percent"), 
                     batch = sce.416b$block)

sce.416b <- sce.416b[, !qc$discard]


# normalization  
library(scran)
sce.416b <- computeSumFactors(sce.416b)
sce.416b <- logNormCounts(sce.416b)
```

## Quantifying per-gene variation  
### Variance of the log-counts  
The simplest approach to quantifying per-gene variation is to simply compute the variance of the log-normalized expression values (referred to as “log-counts” for simplicity) for each gene across all cells in the population (A. T. L. Lun, McCarthy, and Marioni 2016). This has an advantage in that the feature selection is based on the same log-values that are used for later downstream steps. In particular, genes with the largest variances in log-values will contribute the most to the Euclidean distances between cells. By using log-values here, we ensure that our quantitative definition of heterogeneity is consistent throughout the entire analysis.  

Calculation of the per-gene variance is simple but feature selection requires modelling of the mean-variance relationship. As discussed briefly in Section 7.5.1, **the log-transformation does not achieve perfect variance stabilization, which means that the variance of a gene is driven more by its abundance than its underlying biological heterogeneity**. To account for this effect, we use the `modelGeneVar()` function to fit a trend to the variance with respect to abundance across all genes.  

```{r, fig.cap='Variance in the PBMC data set as a function of the mean. Each point represents a gene while the blue line represents the trend fitted to all genes.'}
library(scran)
dec.pbmc <- modelGeneVar(sce.pbmc)

# visualizing the fit
fit.pbmc <- metadata(dec.pbmc)
plot(fit.pbmc$mean, fit.pbmc$var, 
     xlab = "Mean of log-epression", 
     ylab = "Variance of log-expression", 
     pch = 19, 
     cex= 0.5)
curve(fit.pbmc$trend(x), col = "dodgerblue", add = T, lwd = 2)
```

**At any given abundance, we assume that the expression profiles of most genes are dominated by random technical noise (see Section 8.2.3 for details). Under this assumption, our trend represents an estimate of the technical noise as a function of abundance**. We then break down the total variance of each gene into the technical component, i.e., the fitted value of the trend at that gene’s abundance; and the biological component, defined as the difference between the total variance and the technical component. This biological component represents the “interesting” variation for each gene and can be used as the metric for HVG selection.  

```{r}
# ordering by most interesting genes for inspection
dec.pbmc[order(dec.pbmc$bio, decreasing = T), ]
```

> Careful readers will notice that some genes have negative biological components, which have no obvious interpretation and can be ignored in most applications. They are inevitable when fitting a trend to the per-gene variances as approximately half of the genes will lie below the trend.

The trend fit has several useful parameters (see `?fitTrendVar`) that can be tuned for a more appropriate fit. For example, the defaults can occasionally yield an overfitted trend when the few high-abundance genes are also highly variable. In such cases, users can reduce the contribution of those high-abundance genes by turning off density weights.  

```{r}
library(scRNAseq)
# sce.seger <- SegerstolpePancreasData()
sce.seger <- readRDS("data/sce.seger.rds")

# gene annotation
library(AnnotationHub)
edb <- AnnotationHub(localHub = T)[["AH73881"]]
symbols <- rowData(sce.seger)$symbol
ens.id <- mapIds(edb, 
                 keys = symbols, 
                 keytype = "SYMBOL", 
                 column = "GENEID")
ens.id <- ifelse(is.na(ens.id), symbols, ens.id)


# removing duplicated rows
keep <- !duplicated(ens.id)
sce.seger <- sce.seger[keep, ]
rownames(sce.seger) <- ens.id[keep]

# sample annotation
emtab.meta <- colData(sce.seger)[, c("cell type", "disease", "individual", "single cell well quality")]
colnames(emtab.meta) <- c("CellType", "Disease", "Donor", "Quality")
colData(sce.seger) <- emtab.meta

sce.seger$CellType <- gsub(" cell", "", sce.seger$CellType)
sce.seger$CellType <- paste(
    toupper(substr(sce.seger$CellType, 1, 1)), 
    substring(sce.seger$CellType, 2)
)

# quality control
low.qual <- sce.seger$Quality == "low quality cell"


library(scater)
stats <- perCellQCMetrics(sce.seger)
qc <- quickPerCellQC(stats, percent_subsets = "altexps_ERCC_percent", 
                     batch = sce.seger$Donor, 
                     subset =! sce.seger$Donor %in% c("HP1504901", "HP1509101"))

sce.seger <- sce.seger[, !(qc$discard | low.qual)]


# normalization
library(scran)
clusters <- quickCluster(sce.seger)
sce.seger <- computeSumFactors(sce.seger, clusters=clusters)
sce.seger <- logNormCounts(sce.seger) 
```


```{r}
sce.seger <- sce.seger[, sce.seger$Donor == "HP1507101"]
dec.default <- modelGeneVar(sce.seger)
dec.noweight <- modelGeneVar(sce.seger, density.weights = F)

fit.default <- metadata(dec.default)
fit.noweight <- metadata(dec.noweight)

plot(fit.default$mean, fit.default$var, 
     xlab = "Mean of log-expression", 
     ylab = "Variance of log-expression")
curve(fit.default$trend(x), col = "dodgerblue", add = T, lwd = 2)

curve(fit.noweight$trend(x), col = "red", add = T, lwd = 1)

legend("topleft", col = c("dodgerblue", "red"), 
       legend = c("Default", "No weight"), 
       lwd = 2)
```

### Coefficient of variation  
An alternative approach to quantification uses the squared coefficient of variation (CV~2~) of the normalized expression values prior to log-transformation. The CV~2~ is a widely used metric for describing variation in non-negative data and is closely related to the dispersion parameter of the negative binomial distribution in packages like `edgeR` and `DESeq2`. We compute the CV~2~ for each gene in the PBMC dataset using the `modelGeneCV2()` function, which provides a robust implementation of the approach described by Brennecke et al. (2013).  

```{r}
dec.cv2.pbmc <- modelGeneCV2(sce.pbmc)
```

This allows us to model the mean-variance relationship when considering the relevance of each gene. Again, our assumption is that most genes contain random noise and that the trend captures mostly technical variation. Large CV~2~ values that deviate strongly from the trend are likely to represent genes affected by biological structure.  

```{r, fig.cap='CV2 in the PBMC data set as a function of the mean. Each point represents a gene while the blue line represents the fitted trend.'}
fit.cv2.pbmc <- metadata(dec.cv2.pbmc)

plot(fit.cv2.pbmc$mean, fit.cv2.pbmc$cv2, log = "xy")
curve(fit.cv2.pbmc$trend(x), col = "dodgerblue", add = T, lwd = 2)
```

For each gene, we quantify the deviation from the trend in terms of the ratio of its CV~2~ to the fitted value of trend at its abundance. This is more appropriate than the directly subtracting the trend from the CV~2~, as the magnitude of the ratio is not affected by the mean.  

```{r}
dec.cv2.pbmc[order(dec.cv2.pbmc$ratio, decreasing = T), ]
```

**Both the CV~2~ and the variance of log-counts are effective metrics for quantifying variation in gene expression. The CV~2~ tends to give higher rank to low-abundance HVGs driven by upregulation in rare subpopulations, for which the increase in variance on the raw scale is stronger than that on the log-scale**. However, the variation described by the CV~2~ is less directly relevant to downstream procedures operating on the log-counts, and the reliance on the ratio can assign high rank to uninteresting genes with low absolute variance. We prefer the use of the variance of log-counts and will use it in the following sections though many of the same principles apply to procedures based on the CV~2~.  

### Quantifying technical noise  
Strictly speaking, the use of a trend fitted to endogenous genes assumes that the expression profiles of most genes are dominated by random technical noise. In practice, all expressed genes will exhibit some non-zero level of biological variability due to events like transcriptional bursting. This suggests that our estimates of the technical component are likely to be inflated. It would be more appropriate to consider these estimates as technical noise plus “uninteresting” biological variation, under the assumption that most genes are unaffected by the relevant heterogeneity in the population.  

This revised assumption is generally reasonable but may be problematic in some scenarios where many genes at a particular abundance are affected by a biological process. For example, strong upregulation of cell type-specific genes may result in an enrichment of HVGs at high abundances. This would inflate the fitted trend in that abundance interval and compromise the detection of the relevant genes. **We can avoid this problem by fitting a mean-dependent trend to the variance of the spike-in transcripts, if they are available. The premise here is that spike-ins should not be affected by biological variation, so the fitted value of the spike-in trend should represent a better estimate of the technical component for each gene**.  

```{r}
dec.spike.416b <- modelGeneVarWithSpikes(x = sce.416b, spikes = "ERCC")
dec.spike.416b[order(dec.spike.416b$bio, decreasing = T), ]
```

```{r, fig.cap='Variance in the 416B data set as a function of the mean. Each point represents a gene (black) or spike-in transcript (red) and the blue line represents the trend fitted to all spike-ins.'}
plot(dec.spike.416b$mean, dec.spike.416b$total, 
     xlab = "Mean of log-expression", 
     ylab = "Variance of log-expression")

fit.spike.416b <- metadata(dec.spike.416b)
points(fit.spike.416b$mean, fit.spike.416b$var, col = "red", pch = 19)
curve(fit.spike.416b$trend(x), col = "dodgerblue", add = T, lwd = 2)
```

*In the absence of spike-in data, one can attempt to create a trend by making some distributional assumptions about the noise*. For example, UMI counts typically exhibit near-Poisson variation if we only consider technical noise from library preparation and sequencing. This can be used to construct a mean-variance trend in the log-counts with the `modelGeneVarByPoisson()` function. Note the increased residuals of the high-abundance genes, which can be interpreted as the amount of biological variation that was assumed to be “uninteresting” when fitting the gene-based trend.  

```{r}
set.seed(0010101)
dec.pois.pbmc <- modelGeneVarByPoisson(sce.pbmc)
dec.pois.pbmc[order(dec.pois.pbmc$bio, decreasing = T), ]
```

```{r, fig.cap="Variance of normalized log-expression values for each gene in the PBMC dataset, plotted against the mean log-expression. The blue line represents the mean-variance relationship corresponding to Poisson noise."}
plot(dec.pois.pbmc$mean, dec.pois.pbmc$total, 
     pch = 19, 
     xlab = "Mean of log-expression", 
     ylab = "Variance of log-expression")
curve(metadata(dec.pois.pbmc)$trend(x), col = "dodgerblue", add = T, lwd = 2)
```

Interestingly, trends based purely on technical noise tend to yield large biological components for highly-expressed genes. This often includes so-called “house-keeping” genes coding for essential cellular components such as ribosomal proteins, which are considered uninteresting for characterizing cellular heterogeneity. These observations suggest that a more accurate noise model does not necessarily yield a better ranking of HVGs, though one should keep an open mind - house-keeping genes are regularly DE in a variety of conditions (Glare et al. 2002; Nazari, Parham, and Maleki 2015; Guimaraes and Zavolan 2016), and the fact that they have large biological components indicates that there is strong variation across cells that may not be completely irrelevant.  

### Accounting for blocking factors  
#### Fitting block-specific trends  
Data containing multiple batches will often exhibit batch effects (see Chapter 28.8 for more details). We are usually not interested in HVGs that are driven by batch effects. Rather, we want to focus on genes that are highly variable within each batch. This is naturally achieved by performing trend fitting and variance decomposition separately for each batch. We demonstrate this approach by treating each plate (block) in the 416B dataset as a different batch, using the modelGeneVarWithSpikes() function. (The same argument is available in all other variance-modelling functions.)  

```{r}
dec.block.416b <- modelGeneVarWithSpikes(sce.416b, spikes = "ERCC", 
                                         block = sce.416b$block)
dec.block.416b[order(dec.block.416b$bio, decreasing = T), 1:6]
```

The use of a batch-specific trend fit is useful as it accommodates differences in the mean-variance trends between batches. This is especially important if batches exhibit systematic technical differences, e.g., differences in coverage or in the amount of spike-in RNA added. In this case, there are only minor differences between the trends, which indicates that the experiment was tightly replicated across plates. The analysis of each plate yields estimates of the biological and technical components for each gene, which are averaged across plates to take advantage of information from multiple batches.  

```{r, fig.cap="Variance in the 416B data set as a function of the mean after blocking on the plate of origin. Each plot represents the results for a single plate, each point represents a gene (black) or spike-in transcript (red) and the blue line represents the trend fitted to all spike-ins."}
par(mfrow = c(1, 2))
blocked.stats <- dec.block.416b$per.block

for(i in colnames(blocked.stats)){
    current <- blocked.stats[[i]]
    plot(current$mean, current$total, main = i, pch = 16, cex = 0.5, 
         xlab = "Mean of log-expression", 
         ylab = "Variance of log-expression")
    curfit <- metadata(current)
    points(curfit$mean, curfit$var, col = "red", pch = 16)
    curve(curfit$trend(x), col = "dodgerblue", add = T, lwd = 2)
}
```

As an aside, the wave-like shape observed above is typical of the mean-variance trend for log-expression values. (The same wave is present but much less pronounced for UMI data.) A linear increase in the variance is observed as the mean increases from zero, as larger variances are obviously possible when the counts are not all equal to zero. In contrast, the relative contribution of sampling noise decreases at high abundances, resulting in a downward trend. The peak represents the point at which these two competing effects cancel each other out.  

#### Using a design matrix  
The use of block-specific trends is the recommended approach for experiments with a single blocking factor. However, this is not practical for studies involving a large number of blocking factors and/or covariates. In such cases, we can use the `design=` argument to specify a design matrix with uninteresting factors of variation. We illustrate again with the 416B data set, blocking on the plate of origin and oncogene induction. (The same argument is available in `modelGeneVar()` when spike-ins are not available.)  

```{r}
design <- model.matrix( ~ factor(block) + phenotype, colData(sce.416b))
dec.design.416b <- modelGeneVarWithSpikes(sce.416b, spikes = "ERCC", design = design)
dec.design.416b[order(dec.design.416b, decreasing = T), ]
```

**This strategy is simple but somewhat inaccurate as it does not consider the mean expression in each blocking level**. Recall that the technical component is estimated as the fitted value of the trend at the average abundance for each gene. However, the true technical component is the average of the fitted values at the per-block means, which may be quite different for strong batch effects and non-linear mean-variance relationships. The `block=` approach is safer and should be preferred in all situations where it is applicable.  

## Selecting highly variable genes
### Overview  
Once we have quantified the per-gene variation, the next step is to select the subset of HVGs to use in downstream analyses. A larger subset will reduce the risk of discarding interesting biological signal by retaining more potentially relevant genes, at the cost of increasing noise from irrelevant genes that might obscure said signal. It is difficult to determine the optimal trade-off for any given application as noise in one context may be useful signal in another. For example, heterogeneity in T cell activation responses is an interesting phenomena (Richard et al. 2018) but may be irrelevant noise in studies that only care about distinguishing the major immunophenotypes. That said, there are several common strategies that are routinely used to guide HVG selection, which we shall discuss here.  

### Based on the largest metrics  
The simplest HVG selection strategy is to take the top X genes with the largest values for the relevant variance metric. The main advantage of this approach is that the user can directly control the number of genes retained, which ensures that the computational complexity of downstream calculations is easily predicted. For `modelGeneVar()` and `modelGeneVarWithSpikes()`, we would select the genes with the largest biological components:  

```{r}
# taking the top 1000 genes 
hvg.pbmc.var <- getTopHVGs(dec.pbmc, n = 1000)
str(hvg.pbmc.var)
```

For `modelGeneCV2()`(and its relative, `modelGeneCV2WithSpikes()`), this would instead be the genes with the largest ratios:  

```{r}
hvg.pbmc.cv2 <- getTopHVGs(dec.cv2.pbmc, var.field = "ratio", n = 1000)
str(hvg.pbmc.cv2)
```

The choice of X also has a fairly straightforward biological interpretation. Recall our trend-fitting assumption that most genes do not exhibit biological heterogeneity; this implies that they are not differentially expressed between cell types or states in our population. If we quantify this assumption into a statement that, e.g., no more than 5% of genes are differentially expressed, we can naturally set X to 5% of the number of genes. In practice, we usually do not know the proportion of DE genes beforehand so this interpretation just exchanges one unknown for another. Nonetheless, it is still useful as it implies that we should lower X  for less heterogeneous datasets, retaining most of the biological signal without unnecessary noise from irrelevant genes. Conversely, more heterogeneous datasets should use larger values of X to preserve secondary factors of variation beyond those driving the most obvious HVGs.  

The main disadvantage of this approach that it turns HVG selection into a competition between genes, whereby a subset of very highly variable genes can push other informative genes out of the top set. This can be problematic for analyses of highly heterogeneous populations if the loss of important markers prevents the resolution of certain subpopulations. In the most extreme example, consider a situation where a single subpopulation is very different from the others. In such cases, the top set will be dominated by differentially expressed genes involving that distinct subpopulation, compromising resolution of heterogeneity between the other populations. (This can salvaged with a nested analysis, as discussed in Section 10.7, but we would prefer to avoid the problem in the first place.)  

Another possible concern with this approach is the fact that the choice of X is fairly arbitrary, with any value from 500 to 5000 considered “reasonable”. We have chosen `X=1000` in the code above though there is no particular a priori reason for doing so. Our recommendation is to simply pick an arbitrary X and proceed with the rest of the analysis, with the intention of testing other choices later, rather than spending much time worrying about obtaining the “optimal” value.  

### Based on siginificance  
Another approach to feature selection is to set a fixed threshold of one of the metrics. This is most commonly done with the (adjusted) p-value reported by each of the above methods. The p-value for each gene is generated by testing against the null hypothesis that the variance is equal to the trend. For example, we might define our HVGs as all genes that have adjusted p-values below 0.05.  

```{r}
hvg.pbmc.var.2 <- getTopHVGs(dec.pbmc, fdr.threshold = 0.05)
str(hvg.pbmc.var.2)
```

This approach is simple to implement and - if the test holds its size - it controls the false discovery rate (FDR). That is, it returns a subset of genes where the proportion of false positives is expected to be below the specified threshold. This can occasionally be useful in applications where the HVGs themselves are of interest. For example, if we were to use the list of HVGs in further experiments to verify the existence of heterogeneous expression for some of the genes, we would want to control the FDR in that list.  

The downside of this approach is that it is less predictable than the top X strategy. The number of genes returned depends on the type II error rate of the test and the severity of the multiple testing correction. One might obtain no genes or every gene at a given FDR threshold, depending on the circumstances. Moreover, control of the FDR is usually not helpful at this stage of the analysis. We are not interpreting the individual HVGs themselves but are only using them for feature selection prior to downstream steps. There is no reason to think that a 5% threshold on the FDR yields a more suitable compromise between bias and noise compared to the top X selection.  

As an aside, we might consider ranking genes by the p-value instead of the biological component for use in a top X approach. This results in some counterintuitive behavior due to the nature of the underlying hypothesis test, which is based on the ratio of the total variance to the expected technical variance. Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be biologically interesting. Many of the largest ratios are observed in high-abundance genes and are driven by very low technical variance; the total variance is typically modest for such genes, and they do not contribute much to population heterogeneity in absolute terms. (Note that the same can be said of the ratio of CV2 values, as briefly discussed above.)

### Keeping all genes above the trend  
Here, the aim is to only remove the obviously uninteresting genes with variances below the trend. By doing so, we avoid the need to make any judgement calls regarding what level of variation is interesting enough to retain. This approach represents one extreme of the bias-variance trade-off where bias is minimized at the cost of maximizing noise. For `modelGeneVar()`, it equates to keeping all positive biological components:  

```{r}
hvg.pbmc.var.3 <- getTopHVGs(dec.pbmc, var.threshold = 0)
str(hvg.pbmc.var.3)
```

For `modelGeneCV2()`, this involves keeping all ratios above 1:

```{r}
hvg.pbmc.cv2.3 <- getTopHVGs(dec.cv2.pbmc, var.field = "ratio", var.threshold = 1)
str(hvg.pbmc.cv2.3)
```

By retaining all potential biological signal, we give secondary population structure the chance to manifest. This is most useful for rare subpopulations where the relevant markers will not exhibit strong overdispersion owing to the small number of affected cells. It will also preserve a weak but consistent effect across many genes with small biological components; admittedly, though, this is not of major interest in most scRNA-seq studies given the difficulty of experimentally validating population structure in the absence of strong marker genes.  

The obvious cost is that more noise is also captured, which can reduce the resolution of otherwise well-separated populations and mask the secondary signal that we were trying to preserve. The use of more genes also introduces more computational work in each downstream step. This strategy is thus best suited to very heterogeneous populations containing many different cell types (possibly across many datasets that are to be merged, as in Chapter 13) where there is a justified fear of ignoring marker genes for low-abundance subpopulations under a competitive top X approach.  

## Selecting a *priori* genes of interest  
A blunt yet effective feature selection strategy is to use pre-defined sets of interesting genes. The aim is to focus on specific aspects of biological heterogeneity that may be masked by other factors when using unsupervised methods for HVG selection. One example application lies in the dissection of transcriptional changes during the earliest stages of cell fate commitment (Messmer et al. 2019), which may be modest relative to activity in other pathways (e.g., cell cycle, metabolism). Indeed, if our aim is to show that there is no meaningful heterogeneity in a given pathway, we would - at the very least - be obliged to repeat our analysis using only the genes in that pathway to maximize power for detecting such heterogeneity.  

Using scRNA-seq data in this manner is conceptually equivalent to a fluorescence activated cell sorting (FACS) experiment, with the convenience of being able to (re)define the features of interest at any time. For example, in the PBMC dataset, we might use some of the C7 immunologic signatures from MSigDB (Godec et al. 2016) to improve resolution of the various T cell subtypes. We stress that there is no shame in leveraging prior biological knowledge to address specific hypotheses in this manner. We say this because a common refrain in genomics is that the data analysis should be “unbiased”, i.e., free from any biological preconceptions. Attempting to derive biological insight ab initio is admirable but such “biases” are already present at every stage, starting from experimental design (why are we interested in this cell population in the first place?) and continuing through to interpretation of marker genes (Section 11).  

```{r}
library(msigdbr)
c7.sets <- msigdbr(species = "Homo sapiens", category = "C7")
str(c7.sets)
```

```{r}
# using the goldrath sets to distinguish CD8 subtypes  
cd8.sets <- c7.sets[grep("GOLDRATH", c7.sets$gs_name), ]
cd8.genes <- rowData(sce.pbmc)$Symbol %in% cd8.sets$human_gene_symbol
summary(cd8.genes)
```

```{r}
# using GSE11924 to distinguish between T helper subtypes  
th.sets <- c7.sets[grep("GSE11924", c7.sets$gs_name), ]
th.genes <- rowData(sce.pbmc)$Symbol %in% th.sets$human_gene_symbol
summary(th.genes)
```

```{r}
# using GSE11961 to distinguish between B cell subtypes
b.sets <- c7.sets[grep("GSE11961", c7.sets$gs_name), ]
b.genes <- rowData(sce.pbmc)$Symbol %in% b.sets$human_gene_symbol
summary(b.genes)
```

Of course, the downside of focusing on pre-defined genes is that it will limit our capacity to detect novel or unexpected aspects of variation. Thus, this kind of focused analysis should be complementary to (rather than a replacement for) the unsupervised feature selection strategies discussed previously.  

Alternatively, we can invert this reasoning to remove genes that are unlikely to be of interest prior to downstream analyses. This eliminates unwanted variation that could mask relevant biology and interfere with interpretation of the results. Ribosomal protein genes or mitochondrial genes are common candidates for removal, especially in situations with varying levels of cell damage within a population. For immune cell subsets, we might also be inclined to remove immunoglobulin genes and T cell receptor genes for which clonal expression introduces (possibly irrelevant) population structure.  

```{r}
# identifying ribosomal proteins
ribo.discard <- grepl("^RP[SL]\\d+", rownames(sce.pbmc))
summary(ribo.discard)
```

```{r}
# a more curated approach for identifying ribosomal protein genes
c2.sets <- msigdbr(species = "Homo sapiens", category = "C2")
ribo.set <- c2.sets[c2.sets$gs_name == "KEGG_RIBOSOME", ]$human_gene_symbol
ribo.discard <- rownames(sce.pbmc) %in% ribo.set
summary(ribo.discard)
```

```{r}
library(AnnotationHub)
edb <- AnnotationHub()[["AH73881"]]
anno <- select(edb, 
               keys = rowData(sce.pbmc)$ID, 
               keytype = "GENEID", 
               columns = "TXBIOTYPE")

# removing immunoglobulin variable chains
igv.set <- anno$GENEID[anno$TXBIOTYPE %in% c("IG_V_gene", "IG_V_pseudogene")]
igv.discard <- rowData(sce.pbmc)$ID %in% igv.set
summary(igv.discard)
```

```{r}
# remove TCR variable chains  
tcr.set <- anno$GENEID[anno$TXBIOTYPE %in% c("TR_V_gene", "TR_V_pseudogene")]
tcr.discard <- rowData(sce.pbmc)$ID %in% tcr.set
summary(tcr.discard)
```

In practice, we tend to err on the side of caution and abstain from preemptive filtering on biological function until these genes are demonstrably problematic in downstream analyses.  

## Putting is all together  
The few lines of code below will select the top 10% of genes with the highest biological components.  

```{r}
dec.pbmc <- modelGeneVar(sce.pbmc)
chosen <- getTopHVGs(dec.pbmc, prop = 0.1)
str(chosen)
```

We then have several options to enforce our HVG selection on the rest of the analysis.  

1. We can subset the `SingleCellExperiment` to only retain our selection of HVGs. This ensures that downstream methods will only use these genes for their calculations. The downside is that the non-HVGs are discarded from the new `SingleCellExperiment`, making it slightly more inconvenient to interrogate the full dataset fro interesting genes that are not HVGs.  

```{r}
sce.pbmc.hvg <- sce.pbmc[chosen, ]
sce.pbmc.hvg %>% dim()
```


2. We can keep the original `SingleCellExperiment` object and specify the genes to use for downstream functions via an extra argument like `subset.row=`. This is useful if the analysis uses multiple sets of HVGs at different steps, whereby one set of HVGs can be easily swapped for another in specific steps.  

```{r}
# performing PCA only on the chosen HVGs.  
library(scater)
sce.pbmc <- runPCA(sce.pbmc, subset_row = chosen)
reducedDimNames(sce.pbmc)
```

This approach is facilitated by the `rowSubset()` utility, which allows us to easily store one or more sets of interest in our `SingleCellExperiment`. By doing so, we avoid the need to keep track of a separate `chosen` variable and ensure that our HVG set is synchronized with any downstream row subsetting of `sce.pbmc`.  

```{r}
rowSubset(sce.pbmc) <- chosen
rowSubset(sce.pbmc, "HVGs.more") <- getTopHVGs(dec.pbmc, prop = 0.2)
rowSubset(sce.pbmc, "HVGs.less") <- getTopHVGs(dec.pbmc, prop = 0.3)
rowData(sce.pbmc) %>% colnames()
```

It can be inconvenient to repeatedly specify the desired feature set across steps, so some downstream functions will automatically subset to the default `rowSubset()` if present in the `SingleCellExperiment`. However, we find that it is generally safest to be explicit about which set is being used for a particular step.  

3. We can have our cake and eat it too by (ab)using the “alternative Experiment” system in the `SingleCellExperiment` class. Initially designed for storing alternative features like spike-ins or antibody tags, we can instead use it to hold our full dataset while we perform our downstream operations conveniently on the HVG subset. This avoids book-keeping problems in long analyses when the original dataset is not synchronized with the HVG subsetted data.  

```{r}
# recycling the class above 
altExp(sce.pbmc.hvg, "original") <- sce.pbmc
altExpNames(sce.pbmc.hvg)
```

```{r}
# no need for explicit subset_row = specification in downstream operations.  
sce.pbmc.hvg <- runPCA(sce.pbmc.hvg)

# recover original data
sce.pbmc.original <- altExp(sce.pbmc.hvg, "original", withColData = T)
```


# Chapter 9 Dimensionality reduction  
Many scRNA-seq analysis procedures involve comparing cells based on their expression values across multiple genes. For example, clustering aims to identify cells with similar transcriptomic profiles by computing Euclidean distances across genes. In these applications, each individual gene represents a dimension of the data. More intuitively, if we had a scRNA-seq data set with two genes, we could make a two-dimensional plot where each axis represents the expression of one gene and each point in the plot represents a cell. This concept can be extended to data sets with thousands of genes where each cell’s expression profile defines its location in the high-dimensional expression space.  

As the name suggests, dimensionality reduction aims to reduce the number of separate dimensions in the data. This is possible because different genes are correlated if they are affected by the same biological process. Thus, we do not need to store separate information for individual genes, but can instead compress multiple features into a single dimension, e.g., an “eigengene” (Langfelder and Horvath 2007). This reduces computational work in downstream analyses like clustering, as calculations only need to be performed for a few dimensions rather than thousands of genes; reduces noise by averaging across multiple genes to obtain a more precise representation of the patterns in the data; and enables effective plotting of the data, for those of us who are not capable of visualizing more than 3 dimensions.

We will use the Zeisel et al. (2015) dataset to demonstrate the applications of various dimensionality reduction methods in this chapter.  

```{r}
library(scRNAseq)
# sce.zeisel <- ZeiselBrainData()
sce.zeisel <- readRDS("data/sce.zeisel.rds")

library(scater)
sce.zeisel <- aggregateAcrossFeatures(
    x = sce.zeisel, 
    ids = sub("_loc[0-9]+$", "", rownames(sce.zeisel))
)

# gene annotation
library(org.Mm.eg.db)
rowData(sce.zeisel)$Ensembl <- mapIds(org.Mm.eg.db, 
                                      keys = rownames(sce.zeisel), 
                                      keytype = "SYMBOL", 
                                      column = "ENSEMBL")

# quality control
stats <- perCellQCMetrics(sce.zeisel, 
                          subsets = list(Mt = rowData(sce.zeisel)$featureType == "mito"))
qc <- quickPerCellQC(df = stats, 
                     subsets = c("altexps_ERCC_percent", "subsets_Mt_percent"))

qc_ercc <- isOutlier(stats$altexps_ERCC_percent, type = "both", log = T)
qc_mito <- isOutlier(stats$subsets_Mt_percent, type = "both", log = T)
qc <- qc_ercc | qc_mito
qc %>% table

sce.zeisel <- sce.zeisel[, !qc]

# normalization
library(scran)
clusters <- quickCluster(x = sce.zeisel)
sce.zeisel <- computeSumFactors(x = sce.zeisel, cluster = clusters)
sce.zeisel <- logNormCounts(sce.zeisel)

# variance modeling
dec.zeisel <- modelGeneVarWithSpikes(sce.zeisel, spikes = "ERCC")
top.hvgs.zeisel=getTopHVGs(stats = dec.zeisel, prop = 0.1)
str(top.hvgs.zeisel)
sce.zeisel
```

## Principal components analysis  
Principal components analysis (PCA) discovers axes in high-dimensional space that capture the largest amount of variation. This is best understood by imagining each axis as a line. Say we draw a line anywhere, and we move all cells in our data set onto this line by the shortest path. The variance captured by this axis is defined as the variance across cells along that line. In PCA, the first axis (or “principal component”, PC) is chosen such that it captures the greatest variance across cells. The next PC is chosen such that it is orthogonal to the first and captures the greatest remaining amount of variation, and so on.  

By definition, the top PCs capture the dominant factors of heterogeneity in the data set. Thus, we can perform dimensionality reduction by restricting downstream analyses to the top PCs. This strategy is simple, highly effective and widely used throughout the data sciences. It takes advantage of the well-studied theoretical properties of the PCA - namely, that a low-rank approximation formed from the top PCs is the optimal approximation of the original data for a given matrix rank. It also allows us to use a wide range of fast PCA implementations for scalable and efficient data analysis.  

**When applying PCA to scRNA-seq data, our assumption is that biological processes affect multiple genes in a coordinated manner. This means that the earlier PCs are likely to represent biological structure as more variation can be captured by considering the correlated behavior of many genes**. By comparison, random technical or biological noise is expected to affect each gene independently. There is unlikely to be an axis that can capture random variation across many genes, meaning that noise should mostly be concentrated in the later PCs. This motivates the use of the earlier PCs in our downstream analyses, which concentrates the biological signal to simultaneously reduce computational work and remove noise.  

We perform the PCA on the log-normalized expression values using the `runPCA()` function from scater. By default, `runPCA()` will compute the first 50 PCs and store them in the `reducedDims()` of the output `SingleCellExperiment` object, as shown below. Here, we use only the top 2000 genes with the largest biological components to reduce both computational work and high-dimensional random noise. In particular, while PCA is robust to random noise, an excess of it may cause the earlier PCs to capture noise instead of biological structure (Johnstone and Lu 2009). This effect can be mitigated by restricting the PCA to a subset of HVGs, for which we can use any of the strategies described in Chapter 8.  

```{r}
library(scran)
top.zeisel <- getTopHVGs(dec.zeisel, n = 2000)
str(top.zeisel)

library(scater)
set.seed(100)
sce.zeisel <- runPCA(x = sce.zeisel, subset_row = top.zeisel)
reducedDimNames(sce.zeisel)
reducedDim(sce.zeisel, "PCA") %>% dim()
```

For large data sets, greater efficiency is obtained by using approximate SVD algorithms that only compute the top PCs. By default, most PCA-related functions in `scater` and `scran` will use methods from the `irlba` or `rsvd` packages to perform the SVD. We can explicitly specify the SVD algorithm to use by passing an `BiocSingularParam` object (from the `BiocSingular` package) to the `BSPARAM=` argument (see Section 23.2.2 for more details). Many of these approximate algorithms are based on randomization and thus require `set.seed()` to obtain reproducible results.  

```{r}
library(BiocSingular)
set.seed(1000)
sce.zeisel <- runPCA(x = sce.zeisel, 
                     subset_row = top.zeisel, 
                     BSPARAM = RandomParam(), 
                     name = "IRLBA")

reducedDimNames(sce.zeisel)
```

## Choosing the number of PCs  
### Motivation  
How many of the top PCs should we retain for downstream analyses? The choice of the number of PCs d is a decision that is analogous to the choice of the number of HVGs to use. Using more PCs will retain more biological signal at the cost of including more noise that might mask said signal. Much like the choice of the number of HVGs, it is hard to determine whether an “optimal” choice exists for the number of PCs. Even if we put aside the technical variation that is almost always uninteresting, there is no straightforward way to automatically determine which aspects of biological variation are relevant; one analyst’s biological signal may be irrelevant noise to another analyst with a different scientific question.

Most practitioners will simply set d to a “reasonable” but arbitrary value, typically ranging from 10 to 50. This is often satisfactory as the later PCs explain so little variance that their inclusion or omission has no major effect. For example, in the Zeisel dataset, few PCs explain more than 1% of the variance in the entire dataset (Figure 9.1) and using, say, 30 ± 10 PCs would not even amount to four percentage points’ worth of difference in variance. In fact, the main consequence of using more PCs is simply that downstream calculations take longer as they need to compute over more dimensions, but most PC-related calculations are fast enough that this is not a practical concern.  

```{r, fig.cap="Percentage of variance explained by successive PCs in the Zeisel dataset, shown on a log-scale for visualization purposes."}
percent.var <- attr(reducedDim(sce.zeisel), "percentVar")
plot(percent.var, log = "y", xlab = "PC", ylab = "Variance explained (%)")
```

Nonetheless, we will describe some more data-driven strategies to guide a suitable choice of d. These automated choices are best treated as guidelines as they make some strong assumptions about what variation is “interesting”. More diligent readers may consider repeating the analysis with a variety of choices of d to explore other perspectives of the dataset at a different bias-variance trade-off, though this tends to be more work than necessary for most questions.  

### Using the elbow point  
A simple heuristic for choosing d involves identifying the elbow point in the percentage of variance explained by successive PCs. This refers to the “elbow” in the curve of a scree plot.  

```{r, fig.cap="Percentage of variance explained by successive PCs in the Zeisel brain data. The identified elbow point is marked with a red line."}
# percentage of variance explained is tucked away in the attributes.  
percent.var <- attr(reducedDim(sce.zeisel), "percentVar")
chosen.elbow <- PCAtools::findElbowPoint(percent.var)

plot(percent.var, xlab = "PC", ylab = "Variance explained (%)")
abline(v = chosen.elbow, col = "red", lty = 2)
```

Our assumption is that each of the top PCs capturing biological signal should explain much more variance than the remaining PCs. Thus, there should be a sharp drop in the percentage of variance explained when we move past the last “biological” PC. This manifests as an elbow in the scree plot, the location of which serves as a natural choice for d. Once this is identified, we can subset the `reducedDims()` entry to only retain the first d PCs of interest.  

```{r}
# Creating a new entry with only the first 20 PCs, useful if we still need the full set of PCs later.
reducedDim(sce.zeisel, "PCA.elbow") <- reducedDim(sce.zeisel)[, 1:chosen.elbow]
reducedDimNames(sce.zeisel)
```

```{r}
# alternatively, just overwriting the original PCA entry
sce.zeisel.copy <- sce.zeisel
reducedDim(sce.zeisel.copy) <- reducedDim(sce.zeisel.copy)[, 1:chosen.elbow]
reducedDim(sce.zeisel.copy) %>% ncol
```

From a practical perspective, the use of the elbow point tends to retain fewer PCs compared to other methods. The definition of “much more variance” is relative so, in order to be retained, later PCs must explain a amount of variance that is comparable to that explained by the first few PCs. **Strong biological variation in the early PCs will shift the elbow to the left, potentially excluding weaker (but still interesting) variation in the next PCs immediately following the elbow**.  

### Using the technical noise  
Another strategy is to retain all PCs until the percentage of total variation explained reaches some threshold T. For example, we might retain the top set of PCs that explains 80% of the total variation in the data. Of course, it would be pointless to swap one arbitrary parameter d for another T. Instead, we derive a suitable value for T by calculating the proportion of variance in the data that is attributed to the biological component. This is done using the `denoisePCA()` function with the variance modelling results from `modelGeneVarWithSpikes()` or related functions, where T is defined as the ratio of the sum of the biological components to the sum of total variances. To illustrate, we use this strategy to pick the number of PCs in the 10X PBMC dataset.    

```{r}
#--- loading ---#
library(DropletTestFiles)
library(DropletUtils)
fname <- file.path("data/pbmc4k/raw_gene_bc_matrices/GRCh38/")
sce.pbmc <- read10xCounts(fname, col.names=TRUE)

#--- gene-annotation ---#
library(scater)
rownames(sce.pbmc) <- uniquifyFeatureNames(
    rowData(sce.pbmc)$ID, rowData(sce.pbmc)$Symbol)

library(EnsDb.Hsapiens.v86)
location <- mapIds(EnsDb.Hsapiens.v86, keys=rowData(sce.pbmc)$ID, 
    column="SEQNAME", keytype="GENEID")

#--- cell-detection ---#
set.seed(100)
e.out <- emptyDrops(counts(sce.pbmc))
sce.pbmc <- sce.pbmc[,which(e.out$FDR <= 0.001)]

#--- quality-control ---#
stats <- perCellQCMetrics(sce.pbmc, subsets=list(Mito=which(location=="MT")))
high.mito <- isOutlier(stats$subsets_Mito_percent, type="higher")
sce.pbmc <- sce.pbmc[,!high.mito]

#--- normalization ---#
library(scran)
set.seed(1000)
clusters <- quickCluster(sce.pbmc)
sce.pbmc <- computeSumFactors(sce.pbmc, cluster=clusters)
sce.pbmc <- logNormCounts(sce.pbmc)

#--- variance-modelling ---#
set.seed(1001)
dec.pbmc <- modelGeneVarByPoisson(sce.pbmc)
top.pbmc <- getTopHVGs(dec.pbmc, prop=0.1)
```

```{r}
library(scran)
set.seed(111001001)
denoised.pbmc <- denoisePCA(x = sce.pbmc, technical = dec.pbmc, subset.row = top.pbmc)
reducedDim(denoised.pbmc) %>% ncol()
```

The dimensionality of the output represents the lower bound on the number of PCs required to retain all biological variation. This choice of d is motivated by the fact that any fewer PCs will definitely discard some aspect of biological signal. (Of course, the converse is not true; there is no guarantee that the retained PCs capture all of the signal, which is only generally possible if no dimensionality reduction is performed at all.) **From a practical perspective, the `denoisePCA()` approach usually retains more PCs than the elbow point method as the former does not compare PCs to each other and is less likely to discard PCs corresponding to secondary factors of variation. The downside is that many minor aspects of variation may not be interesting (e.g., transcriptional bursting) and their retention would only add irrelevant noise**.  

Note that `denoisePCA()` imposes internal caps on the number of PCs that can be chosen in this manner. By default, the number is bounded within the “reasonable” limits of 5 and 50 to avoid selection of too few PCs (when technical noise is high relative to biological variation) or too many PCs (when technical noise is very low). For example, applying this function to the Zeisel brain data hits the upper limit:  

```{r}
set.seed(001001001)
denoised.zeisel <- denoisePCA(sce.zeisel, technical = dec.zeisel, subset.row = top.zeisel)
reducedDim(denoised.zeisel) %>% ncol
```

This method also tends to perform best when the mean-variance trend reflects the actual technical noise, i.e., estimated by `modelGeneVarByPoisson()` or `modelGeneVarWithSpikes()` instead of `modelGeneVar()` (Chapter 8). Variance modelling results from `modelGeneVar()` tend to understate the actual biological variation, especially in highly heterogeneous datasets where secondary factors of variation inflate the fitted values of the trend. Fewer PCs are subsequently retained because T is artificially lowered, as evidenced by `denoisePCA()` returning the lower limit of 5 PCs for the PBMC dataset:  

```{r}
dec.pbmc2 <- modelGeneVar(sce.pbmc)
denoised.pbmc2 <- denoisePCA(sce.pbmc, technical = dec.pbmc2, subset.row = top.pbmc)
reducedDim(denoised.pbmc2) %>% ncol
```

### Based on population structure  
Yet another method to choose `d` uses information about the number of subpopulations in the data. Consider a situation where each subpopulation differs from the others along a different axis in the high-dimensional space (e.g., because it is defined by a unique set of marker genes). This suggests that we should set `d` to the number of unique subpopulations minus 1, which guarantees separation of all subpopulations while retaining as few dimensions (and noise) as possible. We can use this reasoning to loosely motivate an a priori choice for `d` - for example, if we expect around 10 different cell types in our population, we would set `d≈10`.  

In practice, the number of subpopulations is usually not known in advance. Rather, we use a heuristic approach that uses the number of clusters as a proxy for the number of subpopulations. We perform clustering (graph-based by default, see Chapter 10) on the first  
`d∗` PCs and only consider the values of `d∗` that yield no more than `d∗ + 1` clusters. If we detect more clusters with fewer dimensions, we consider this to represent overclustering rather than distinct subpopulations, assuming that multiple subpopulations should not be distinguishable on the same axes. We test a range of `d∗` and set `d` to the value that maximizes the number of clusters while satisfying the above condition. This attempts to capture as many distinct (putative) subpopulations as possible by retaining biological signal in later PCs, up until the point that the additional noise reduces resolution.  

```{r}
pcs <- reducedDim(sce.zeisel, type = "PCA")
choices <- getClusteredPCs(pcs = pcs)
val <- metadata(choices)$chosen
```

```{r, fig.cap="Number of clusters detected in the Zeisel brain dataset as a function of the number of PCs. The red unbroken line represents the theoretical upper constraint on the number of clusters, while the grey dashed line is the number of PCs suggested by `getClusteredPCs()`."}
plot(choices$n.pcs, choices$n.clusters, 
     xlab = "Number of PCs", 
     ylab = "Number of Clusters")
abline(a = 1, b = 1, col = "red")
abline(v = val, col = "grey80", lty = 2)
```

We subset the PC matrix by column to retain the first `d` PCs and assign the subsetted matrix back into our `SingleCellExperiment` object. Downstream applications that use the "PCA.clust" results in `sce.zeisel` will subsequently operate on the chosen PCs only.  

```{r}
reducedDim(sce.zeisel, "PCA.clust") <- pcs[, 1:val]
```

This strategy is the most pragmatic as it directly addresses the role of the bias-variance trade-off in downstream analyses, specifically clustering. There is no need to preserve biological signal beyond what is distinguishable in later steps. However, it involves strong assumptions about the nature of the biological differences between subpopulations - and indeed, discrete subpopulations may not even exist in studies of continuous processes like differentiation. It also requires repeated applications of the clustering procedure on increasing number of PCs, which may be computational expensive.  

### Using random matrix theory  
We consider the observed (log-)expression matrix to be the sum of (i) a low-rank matrix containing the true biological signal for each cell and (ii) a random matrix representing the technical noise in the data. Under this interpretation, we can use random matrix theory to guide the choice of the number of PCs based on the properties of the noise matrix.  

**The Marchenko-Pastur (MP) distribution** defines an upper bound on the singular values of a matrix with random i.i.d. entries. Thus, all PCs associated with larger singular values are likely to contain real biological structure - or at least, signal beyond that expected by noise - and should be retained (Shekhar et al. 2016). We can implement this scheme using the `chooseMarchenkoPastur()` function from the `PCAtools` package, given the dimensionality of the matrix used for the PCA (noting that we only used the HVG subset); the variance explained by each PC (not the percentage); and the variance of the noise matrix derived from our previous variance decomposition results.  

The Marchenko-Pastur (MP) distribution defines an upper bound on the singular values of a matrix with random i.i.d. entries. Thus, all PCs associated with larger singular values are likely to contain real biological structure - or at least, signal beyond that expected by noise - and should be retained (Shekhar et al. 2016). We can implement this scheme using the `chooseMarchenkoPastur()` function from the `PCAtools` package, given the dimensionality of the matrix used for the PCA (noting that we only used the HVG subset); the variance explained by each PC (not the percentage); and the variance of the noise matrix derived from our previous variance decomposition results.  

```{r}
# generating more PCs for demonstration purposes
set.seed(10100101)
sce.zeisel2 <- runPCA(sce.zeisel, subset_row = top.hvgs, ncomponents = 200)

mp.choice <- PCAtools::chooseMarchenkoPastur(
    .dim = c(length(top.hvgs), ncol(sce.zeisel2)), 
    var.explained = attr(reducedDim(sce.zeisel2), "varExplained"), 
    noise = median(dec.zeisel[top.hvgs, "tech"])
)

mp.choice
```

We can then subset the PC coordinate matrix by the first `mp.choice` columns as previously demonstrated. It is best to treat this as a guideline only; PCs below the MP limit are not necessarily uninteresting, especially in noisy datasets where the higher `noise` drives a more aggressive choice of `d`. Conversely, many PCs above the limit may not be relevant if they are driven by uninteresting biological processes like transcriptional bursting, cell cycle or metabolic variation. Morever, the use of the MP distribution is not entirely justified here as the noise distribution differs by abundance for each gene and by sequencing depth for each cell.  

In a similar vein, Horn’s parallel analysis is commonly used to pick the number of PCs to retain in factor analysis. This involves randomizing the input matrix, repeating the PCA and creating a scree plot of the PCs of the randomized matrix. The desired number of PCs is then chosen based on the intersection of the randomized scree plot with that of the original matrix. 

Here, the reasoning is that PCs are unlikely to be interesting if they explain less variance that that of the corresponding PC of a random matrix. Note that this differs from the MP approach as we are not using the upper bound of randomized singular values to threshold the original PCs.  

```{r}
set.seed(100010)
horn <- PCAtools::parallelPCA(mat = logcounts(sce.zeisel)[top.hvgs, ], 
                              BSPARAM = BiocSingular::IrlbaParam(), 
                              niters = 10)

horn$n
```

```{r, fig.cap="Percentage of variance explained by each PC in the original matrix (black) and the PCs in the randomized matrix (grey) across several randomization iterations. The red line marks the chosen number of PCs."}
plot(horn$original$variance, type = "b", log = "y", pch = 16)
permuted <- horn$permuted
for(i in seq_len(ncol(permuted))){
    points(permuted[, i], col = "grey80", pch = 16)
    lines(permuted[, i], col = "grey80", pch = 16)
}
abline(v = horn$n, col = "blue", lty = 2)
```

The `parallelPCA()` function helpfully emits the PC coordinates in `horn$original$rotated`, which we can subset by `horn$n` and add to the `reducedDims()` of our `SingleCellExperiment`. Parallel analysis is reasonably intuitive (as random matrix methods go) and avoids any i.i.d. assumption across genes. However, its obvious disadvantage is the not-insignificant computational cost of randomizing and repeating the PCA. One can also debate whether the scree plot of the randomized matrix is even comparable to that of the original, given that the former includes biological variation and thus cannot be interpreted as purely technical noise. This manifests in Figure 9.4 as a consistently higher curve for the randomized matrix due to the redistribution of biological variation to the later PCs.  

Another approach is based on optimizing the reconstruction error of the low-rank representation (**???**). Recall that PCA produces both the matrix of per-cell coordinates and a rotation matrix of per-gene loadings, the product of which recovers the original log-expression matrix. If we subset these two matrices to the first `d` dimensions, the product of the resulting submatrices serves as an approximation of the original matrix. Under certain conditions, the difference between this approximation and the true low-rank signal (i.e., sans the noise matrix) has a defined mininum at a certain number of dimensions. This minimum can be defined using the `chooseGavishDonoho()` function from `PCAtools` as shown below.

```{r}
gv.choice <- PCAtools::chooseGavishDonoho(
    .dim = c(length(top.hvgs), ncol(sce.zeisel2)), 
    var.explained = attr(reducedDim(sce.zeisel2), "varExplained"), 
    noise = median(dec.zeisel[top.hvgs, "tech"])
)

gv.choice
```

The Gavish-Donoho method is appealing as, unlike the other approaches for choosing `d`, the concept of the optimum is rigorously defined. By minimizing the reconstruction error, we can most accurately represent the true biological variation in terms of the distances between cells in PC space. However, there remains some room for difference between “optimal” and “useful”; for example, noisy datasets may find themselves with very low `d` as including more PCs will only ever increase reconstruction error, regardless of whether they contain relevant biological variation. This approach is also dependent on some strong i.i.d. assumptions about the noise matrix.  

## Count-based dimensionality reduction  
For count matrices, correspondence analysis (CA) is a natural approach to dimensionality reduction. In this procedure, we compute an expected value for each entry in the matrix based on the per-gene abundance and size factors. Each count is converted into a standardized residual in a manner analogous to the calculation of the statistic in Pearson’s chi-squared tests, i.e., subtraction of the expected value and division by its square root. An SVD is then applied on this matrix of residuals to obtain the necessary low-dimensional coordinates for each cell. To demonstrate, we use the corral package to compute CA factors for the Zeisel dataset.  

```{r}
library(corral)

sce.corral <- corral_sce(sce.zeisel, 
                         subset_row = top.hvgs, 
                         col.w = sizeFactors(sce.zeisel))

reducedDim(sce.corral, "corral") %>% dim
```

The major advantage of CA is that it avoids difficulties with the mean-variance relationship upon transformation (Section 7.5.1). If two cells have the same expression profile but differences in their total counts, CA will return the same expected location for both cells; this avoids artifacts observed in PCA on log-transformed counts (Figure 9.5). However, CA is more sensitive to overdispersion in the random noise due to the nature of its standardization. This may cause some problems in some datasets where the CA factors may be driven by a few genes with random expression rather than the underlying biological structure.  

```{r}
library(BiocFileCache)

load("data/mRNAmix_qc.RData")
sce.8qc <- sce8_qc
sce.8qc$mix <- factor(sce.8qc$mix)

# choosing some HVGs for PCA
sce.8qc <- logNormCounts(sce.8qc)
dec.8qc <- modelGeneVar(sce.8qc)
hvgs.8qc <- getTopHVGs(dec.8qc, n = 1000)
sce.8qc <- runPCA(sce.8qc, subset_row = hvgs.8qc)

# by comparison, corral operates on the raw counts
sce.8qc <- corral_sce(sce.8qc, 
                      subset_row = hvgs.8qc, 
                      col.w = sizeFactors(sce.8qc))

```

```{r, fig.cap="Dimensionality reduction results of all pool-and-split libraries in the SORT-seq CellBench data, computed by a PCA on the log-normalized expression values (left) or using the corral package (right). Each point represents a library and is colored by the mixing ratio used to construct it."}
library(patchwork)

p1 <- plotPCA(sce.8qc, colour_by = "mix") + ggtitle("PCA")
p2 <- plotReducedDim(sce.8qc, dimred = "corral", colour_by = "mix") + ggtitle("corral")

p1 | p2
```

## Dimensionality reduction for visualization  
### Motivation  
Another application of dimensionality reduction is to compress the data into 2 (sometimes 3) dimensions for plotting. This serves a separate purpose to the PCA-based dimensionality reduction described above. Algorithms are more than happy to operate on 10-50 PCs, but these are still too many dimensions for human comprehension. Further dimensionality reduction strategies are required to pack the most salient features of the data into 2 or 3 dimensions, which we will discuss below.  

#### Visualizing with PCA  
```{r, fig.cap="PCA plot of the first two PCs in the Zeisel brain data. Each point is a cell, coloured according to the annotation provided by the original authors."}
sce.zeisel <- runPCA(sce.zeisel, subset_row = top.hvgs.zeisel)
plotReducedDim(sce.zeisel, dimred = "PCA", colour_by = "level1class")
```

The problem is that PCA is a linear technique, i.e., only variation along a line in high-dimensional space is captured by each PC. As such, it cannot efficiently pack differences in d dimensions into the first 2 PCs. This is demonstrated in Figure 9.6 where the top two PCs fail to resolve some subpopulations identified by Zeisel et al. (2015). If the first PC is devoted to resolving the biggest difference between subpopulations, and the second PC is devoted to resolving the next biggest difference, then the remaining differences will not be visible in the plot.  









# Resources  
- http://bioconductor.org/books/release/OSCA/  



