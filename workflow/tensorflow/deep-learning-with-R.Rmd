---
title: "deep-learning-with-R"
author: "yincy"
date: "3/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## The IMDB dataset  
```{r}
library(keras)
```


### Preparing the data  
```{r, cache=TRUE}
imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
```

```{r}
str(train_data[[1]])
str(train_labels)
```

```{r}
sapply(train_data, max) %>% max()
```

```{r}
word_index <- dataset_imdb_word_index()

reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index

decode_review <- sapply(train_data[[1]], function(x){
  word <- if(x >= 3) reverse_word_index[[as.character(x - 3)]]
  if(!is.null(word)) word else "?"
}) %>% stringr::str_c(collapse = " ")
decode_review
```


```{r}
decode_review <- function(text){
  word_index <- dataset_imdb_word_index()
  reverse_word_index <- names(word_index)
  names(reverse_word_index) <- unlist(word_index, use.names = F)
  word <- c()
  for(i in text){
    if(i > 3){
      word <- append(word, reverse_word_index[as.character(i - 3)])
    }else{
      word <- append(word, "?")
    }
  }
  return(word %>% stringr::str_c(collapse = " "))
}

decode_review(train_data[[1]])
```


Encoding the integer sequences into a binary matrix  
```{r}
vectorize_sequence <- function(sequences, dimension = 10000){
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for(i in 1:length(sequences)){
    results[i, sequences[[i]]] <- 1
  }
  results
}
```


```{r}
x_train <- vectorize_sequence(train_data)
x_test <- vectorize_sequence(test_data)
```


convert labels from integer to numeric 
```{r}
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```


### Building network  
```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```


Compiling the model  
```{r}
model %>% compile(optimizer = optimizer_rmsprop(), 
                  loss = loss_binary_crossentropy, 
                  metrics = metric_binary_accuracy)
```


### Validating  
```{r}
val_indices <- 1:10000

x_val <- x_train[val_indices, ]
partial_x_train <- x_train[-val_indices, ]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```

Training the model  
```{r}
history <- model %>% fit(
  partial_x_train, 
  partial_y_train, 
  epochs = 20, 
  batch_size = 512, 
  validation_data = list(x_val, y_val)
)
```

```{r}
str(history)
```

```{r, message=FALSE}
plot(history)
```

```{r}
model <-  keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid") 

model %>% compile(
    optimizer = optimizer_rmsprop(), 
    loss = loss_binary_crossentropy, 
    metrics = metric_binary_accuracy
  ) 

model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)

results <- model %>% evaluate(x_test, y_test)
```

```{r}
model %>% predict(x_test[1:10, ])
```


### Further experiments  
```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "tanh", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "tanh") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = loss_binary_crossentropy,
  metrics = metric_binary_accuracy
)

model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)

model %>% predict(x_test[1:10, ])
```


## Prediction house price: a regression example   
```{r, cache=TRUE}
dataset <- dataset_boston_housing()
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset
```

404 train samples and 102 test samples  
```{r}
str(train_data)
str(test_data)
```

```{r}
str(train_targets)
str(test_targets)
```


### Preparing the data  
normalizaing the data  
You should never use in your workflow any quantity computed on the test data, even for something as simple as data normalization.  
```{r}
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)

train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
```

### Building network  
In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting.  
```{r}
build_model <- function(){
  model <- keras_model_sequential() %>% 
    layer_dense(units = 64, activation = "relu", input_shape = dim(train_data)[2]) %>% 
    layer_dense(units = 64, activation = "relu") %>% 
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = optimizer_rmsprop(), 
    loss = loss_mean_squared_error, 
    metrics = metric_mean_absolute_error
  )
}
```


### Validating your approach using K-fold validation  
```{r}
k <- 4
indices <- sample(1:nrow(train_data))
folds <- cut(indices, k, labels = F)

num_epochs <- 50
all_mae_history <- NULL
for(i in 1:k){
  cat("processing fold #", i, "\n")
  
  val_indices <- which(folds == i, arr.ind = T)
  val_data <- train_data[val_indices, ]
  val_targets <- train_targets[val_indices]
  
  partial_train_data <- train_data[-val_indices, ]
  partial_train_targets <- train_targets[-val_indices]
  
  model <- build_model()
  
  history <- model %>% fit(partial_train_data, 
                partial_train_targets, 
                epochs = num_epochs, 
                batch_size = 1, 
                verbose = 0)
  
  mae_history <- history$metrics$mean_absolute_error
  all_mae_history <- rbind(all_mae_history, mae_history)
}
```

```{r}
average_mae_history <- data.frame(
  epoch = seq(1:ncol(all_mae_history)), 
  validation_mae = apply(all_mae_history, 2, mean)
)
```


```{r, message=FALSE}
library(ggplot2)

average_mae_history %>% 
  ggplot(aes(epoch, validation_mae)) +
  geom_line()
```

according to the above validation training the final model  
```{r}
model <- build_model()

model %>% fit(train_data, 
              train_targets, 
              epochs = 80, 
              batch_size = 16, 
              verbose = 0)

result <- model %>% evaluate(test_data, test_targets)
model %>% predict(test_data[1:10, ])
```


# Chapter 4 Fundamentals of machine learning  
## Machine-learning algorithms generally fall four broad categories  
- Supervised learning  

- Unsupervised learning  

- Reinforcement learning  

- Self-supervised learning  


## Evaluating machine-learning models  
- Simple hold-out validation  
- k-fold validation  
- iterated k-fold validation with shuffling: Machine-learning algorithms generally fall four broad categories.  

**There are 4 most common ways to prevent overfitting in neural network**  

- get more training data  
- reduce the capacity of the network  
- add weight regularization  
- add dropout  


# Chapter 5 Deep learning for computer vision  
```{r}
library(keras)
```


```{r}
model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")
```
a convnet takes as input tensors of shape (image_height, image_width, image_channels) (not including the batch dimension).  


```{r}
model <- model %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")
```


```{r, cache=TRUE}
mnist <- dataset_mnist()
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
```

```{r}
train_images <- array_reshape(train_images, dim = c(60000, 28, 28, 1))
test_images <- array_reshape(test_images, dim = c(10000, 28, 28, 1))

train_images <- train_images / 255 
test_images <- test_images / 255

train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```


```{r}
model %>% compile(optimizer = "rmsprop", 
                  loss = "categorical_crossentropy", 
                  metrics = c("accuracy"))

model %>% fit(
  train_images, train_labels, 
  epochs = 5, 
  batch_size = 64
)
```


evalutate the model   
```{r}
results <- model %>% 
  evaluate(test_images, test_labels)
```


## Training a convent from scrach on a small dataset  






# Chapter 6 Deep learning for text and sequences  
```{r word-level one-hot encoding}
samples <- c("The cat sat on the mat.", "The dog ate my homework.")
token_index <- list()

for(sample in samples){
  for(word in strsplit(sample, " ")[[1]]){
    if(!word %in% names(token_index)){
      token_index[[word]] <- length(token_index) + 2
    }
  }
}

max_length <- 10
results <- array(0, dim = c(length(samples), max_length, max(as.integer(token_index))))

for(i in 1:length(samples)){
  sample <- samples[[i]]
  words <- head(strsplit(sample, " ")[[1]], n = max_length)
  for(j in 1:length(words)){
    index = token_index[[words[[j]]]]
    results[[i, j, index]] <- 1
  }
}
```


```{r Character-level one-hot encoding}
samples <- c("The cat sat on the mat.", "The dog ate my homework.")

ascii_tokens <- c("", sapply(as.raw(c(32:126)), rawToChar))
token_index <- c(1:length(ascii_tokens))
names(token_index) <- ascii_tokens

max_length <- 50

results <- array(0, dim = c(length(samples), max_length, length(token_index)))

for(i in 1:length(samples)){
  sample <- samples[[i]]
  characters <- strsplit(sample, "")[[1]]
  for(j in 1:length(characters)){
    character <- characters[[j]]
    results[i, j, token_index[[character]]] <- 1
  }
}
```














