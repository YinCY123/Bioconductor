---
title: "Single-Cell Analysis With Bioconductor"
author: "yincy"
date: "3/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(SingleCellExperiment)
library(scater)
library(scran)
library(uwot)
library(Rtsne)
library(scRNAseq)
```

[From this web site](https://osca.bioconductor.org/)  

# Chapter 4 Data Structure  
```{r}
knitr::include_graphics(path = "figures/SingleCellExperiment-strucutre.PNG")
```


## 4.2 Storing primary experimental data  
```{r}
count_matrix <- data.frame(
    cell_1 = rpois(n = 10, 10), 
    cell_2 = rpois(n = 10, 10), 
    cell_3 = rpois(n = 10, 30)
)

rownames(count_matrix) <- paste("gene_", 1:10, sep = "")
count_matrix <- as.matrix(count_matrix)
```

The SingleCellExperiment class inherits from the SummarizedExperiment class with several additional slots.  

- `reducedDims`: A SimpleList containing matrices of cell coordinates.  
- `int_elementMetadata`: A DataFrame containing internal row metadata (for each genomic features).  
- `int_colData`: A DataFrame containing internal column metadata (for each cell).  
- `int_metadata`: A list containing internal experiment metadata.  

The additional reducedDims slot allows storage of results from multiple dimensionality reduction methods, e.g., PCA or t-SNE. Each element of the SimpleList should be a matrix of coordinates for all cells from one reduction method. The number of rows of each matrix should be equal to the number of cells in the SingleCellExperiment object.  

```{r}
sce <- SingleCellExperiment(
    assays = list(counts = count_matrix)
)
```

```{r}
sce
```

```{r}
assay(sce, 1)
# or assay(sce, "counts")
```

```{r}
sce <- scater::normalizeSCE(sce)
```

```{r}
assays(sce)
```

```{r}
assay(sce, "logcounts")
```


```{r}
logcounts(sce)
```

```{r}
counts_100 <- counts(sce) + 100
assay(sce, "counts_100") <- counts_100

assays(sce)
assay(sce, "counts_100")
```

```{r}
cell_metadata <- data.frame(batch = c(1, 1, 2))
rownames(cell_metadata) <- paste("cell_", 1:3, sep = "")
```


```{r}
sce <- SingleCellExperiment(assays = list(counts = count_matrix), 
                            colData = cell_metadata)
```

```{r}
colData(sce)
```

```{r}
sce$batch
```

```{r}
sce <- scater::addPerCellQC(sce)
colData(sce)
```

```{r}
sce$more_stuff <- runif(ncol(sce))
colData(sce)
```

A common operation with colData is to use its values for subsetting.  
```{r}
sce[, sce$batch == 1]
```


```{r}
rowRanges(sce) # to store GRanges or GRangesList object
```

```{r}
rowqc <- data.frame(
    mean = apply(assay(sce, "counts"), 1, mean, na.rm = T), 
    detected = apply(assay(sce, "counts"), 1, function(x)mean(x > 0) * 100)
)

rowqc

# or using this way 
sce <- scater::addPerFeatureQC(sce)
```

```{r}
rowData(sce) <- rowqc
```

```{r}
sce
```

```{r}
sce[c("gene_1", "gene_3"), ]
```

```{r}
sce[c(1, 3),] # same as above
```


Some other meta data do not directly related to cells or samples can be stored in `metadata` slot.  
```{r}
my_genes <- c("gene_1", "gene_5")
metadata(sce) <- list(favorite_genes = my_genes)
metadata(sce)
```

```{r}
your_geen <- c("gene_4", "gene_8")
metadata(sce)$your_gene <- your_geen

metadata(sce)
```

The `reducedDims` slot is specially designed to store reduced dimensionality representations of the primary data obtained by methods such as PCA and t-SNE (see Chapter 9 for more details). This slot contains a list of numeric matrices of low-reduced representations of the primary data, where the rows represent the columns of the primary data (i.e., cells), and columns represent the dimensions.  

```{r}
sce <- logNormCounts(sce)
sce <- runPCA(sce)

x <- reducedDim(sce, "PCA")
attr(x, "percentVar")
```

```{r}
sce <- runTSNE(sce, perplexity = 0.1)

reducedDim(sce, "TSNE")
```

```{r}
u <- uwot::umap(t(logcounts(sce)), n_neighbors = 2)
reducedDim(sce, "UMAP") <- u
reducedDims(sce)
```

```{r}
reducedDim(sce, "UMAP")
```


```{r}
sce <- scran::computeSumFactors(sce)
```


# Chapter 5 Overview  
```{r, fig.cap="scRNA-seq analysis workflow"}
knitr::include_graphics(path = "figures/SingeCellExperiment-analysis-flow.PNG")
```

## 5.2 Experimental Design  
**Sequencing technology**:  

- Droplet-based: 10X Genomics, inDrop, Drop-seq  

- Plate-based with unique molecular identifers (UMIs): CEL-seq, MARS-seq  

- Plate-base with reads: Smart-seq2  

- Other: sci-RNA-seq, Seq-Well  


## Obtaining a count matrix  
- For 10X Genomics data, the CellRanger software suite provides a custom pipeline to obtain a count matrix. This uses STAR to align reads to the reference genome and then counts the number of unique UMIs mapped to each gene.  

- Pseudo-alignment methods such as alevin can be used to obtain a count matrix from the same data with greater efficiency. This avoids the need for explicit alignment, which reduces the compute time and memory usage.  

- For other highly multiplexed protocols, the `scPipe` package provides a more general pipeline for processing scRNA-seq data. This uses the `Rsubread` aligner to align reads and then counts UMIs per gene.  

- For CEL-seq or CEL-seq2 data, the `scruff` package provides a dedicated pipeline for quantification.  

- For read-based protocols, we can generally re-use the same pipelines for processing bulk RNA-seq data.  

- For any data involving spike-in transcripts, the spike-in sequences should be included as part of the reference genome during alignment and quantification.  

After quantification, we import the count matrix into R and create a SingleCellExperiment object. This can be done with base methods (e.g., read.table()) followed by applying the SingleCellExperiment() constructor. Alternatively, for specific file formats, we can use dedicated methods from the `DropletUtils` (for 10X data) or `tximport`/`tximeta` packages (for pseudo-alignment methods).  


## Data processing and downstream analysis  
In the simplest case, the workflow has the following form:  

1. **We compute quality control metrics to remove low-quality cells that would interfere with downstream analyses**. These cells may have been damaged during processing or may not have been fully captured by the sequencing protocol. Common metrics includes the total counts per cell, the proportion of spike-in or mitochondrial reads and the number of detected features.  

2. **We convert the counts into normalized expression values to eliminate cell-specific biases (e.g., in capture efficiency)**. This allows us to perform explicit comparisons across cells in downstream steps like clustering. We also apply a transformation, typically log, to adjust for the mean-variance relationship.  

3. **We perform feature selection to pick a subset of interesting features for downstream analysis**. This is done by modelling the variance across cells for each gene and retaining genes that are highly variable. The aim is to reduce computational overhead and noise from uninteresting genes.  

4. **We apply dimensionality reduction to compact the data and further reduce noise**. Principal components analysis is typically used to obtain an initial low-rank representation for more computational work, followed by more aggressive methods like t-stochastic neighbor embedding for visualization purposes.  

5. **We cluster cells into groups according to similarities in their (normalized) expression profiles**. This aims to obtain groupings that serve as empirical proxies for distinct biological states. We typically interpret these groupings by identifying differentially expressed marker genes between clusters.  


# Chapter 6 Quality Control  
Low-quality libraries in scRNA-seq data can arise from a variety of sources such as cell damage during dissociation or failure in library preparation (e.g., inefficient reverse transcription or PCR amplification). These usually manifest as “cells” with low total counts, few expressed genes and high mitochondrial or spike-in proportions. These low-quality libraries are problematic as they can contribute to misleading results in downstream analyses.  

- They form their own distinct cluster(s), complicating interpretation of the results. This is most obviously driven by increased mitochondrial proportions or enrichment for nuclear RNAs after cell damage. In the worst case, low-quality libraries generated from different cell types can cluster together based on similarities in the damage-induced expression profiles, creating artificial intermediate states or trajectories between otherwise distinct subpopulations. Additionally, very small libraries can form their own clusters due to shifts in the mean upon transformation.  

- They distort the characterization of population heterogeneity during variance estimation or principal components analysis. The first few principal components will capture differences in quality rather than biology, reducing the effectiveness of dimensionality reduction. Similarly, genes with the largest variances will be driven by differences between low- and high-quality cells. The most obvious example involves low-quality libraries with very low counts where scaling normalization inflates the apparent variance of genes that happen to have a non-zero count in those libraries.  

- They contain genes that appear to be strongly “upregulated” due to aggressive scaling to normalize for small library sizes. This is most problematic for contaminating transcripts (e.g., from the ambient solution) that are present in all libraries at low but constant levels. Increased scaling in low-quality libraries transforms small counts for these transcripts in large normalized expression values, resulting in apparent upregulation compared to other cells. This can be misleading as the affected genes are often biologically sensible but are actually expressed in another subpopulation.  

To avoid - or at least mitigate - these problems, we need to remove these cells at the start of the analysis. This step is commonly referred to as quality control (QC) on the cells.  

```{r}
library(biomaRt)
counts <- readRDS(file = "/home/yincy/git/Bioconductor/Books/Single-Cell-analysis-with-Bioconductor/counts.416b.rds")
cd <- readRDS(file = "/home/yincy/git/Bioconductor/Books/Single-Cell-analysis-with-Bioconductor/colData.416b.rds")
rd <- readRDS(file = "/home/yincy/git/Bioconductor/Books/Single-Cell-analysis-with-Bioconductor/rowData.416b.rds")

cgs <- rownames(counts)
genes <- genes(EnsDb.Mmusculus.v79)
genes <- genes[genes$gene_id %in% cgs]

sce.416b <- SingleCellExperiment(list(counts), 
                                 colData = cd,
                                 rowData = rd)
cse.416b <- LunSpikeInData(which = "416b")
sce.416b$block <- factor(sce.416b$block)
```


## Choice of QC metrics  
We use several common QC metrics to identify low-quality cells based on their expression profiles. These metrics are described below in terms of reads for SMART-seq2 data, but the same definitions apply to UMI data generated by other technologies like MARS-seq and droplet-based protocols.  

- **The library size** is defined as the total sum of counts across all relevant features for each cell. Here, we will consider the relevant features to be the endogenous genes. Cells with small library sizes are of low quality as the RNA has been lost at some point during library preparation, either due to cell lysis or inefficient cDNA capture and amplification.  

- **The number of expressed features** in each cell is defined as the number of endogenous genes with non-zero counts for that cell. Any cell with very few expressed genes is likely to be of poor quality as the diverse transcript population has not been successfully captured.  

- **The proportion of reads mapped to spike-in transcripts** is calculated relative to the total count across all features (including spike-ins) for each cell. As the same amount of spike-in RNA should have been added to each cell, any enrichment in spike-in counts is symptomatic of loss of endogenous RNA. Thus, high proportions are indicative of poor-quality cells where endogenous RNA has been lost due to, e.g., partial cell lysis or RNA degradation during dissociation.  

- In the absence of spike-in transcripts, **the proportion of reads mapped to genes in the mitochondrial genome** can be used. High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), presumably because of loss of cytoplasmic RNA from perforated cells. The reasoning is that, in the presence of modest damage, the holes in the cell membrane permit efflux of individual transcript molecules but are too small to allow mitochondria to escape, leading to a relative enrichment of mitochondrial transcripts.  

For each cell, we calculate these QC metrics using the perCellQCMetrics() function from the `scater` package (McCarthy et al. 2017). The `sum` column contains the total count for each cell, the `detected` column contains the number of detected genes, `subsets_Mito_percent` contains the percentage of reads mapped to mitochondrial transcripts (based on Ensembl annotation) and `altexps_ERCC_percent` contains the percentage of reads mapped to ERCC transcripts.  

```{r}
# retrieving the mitochondrial transcripts using genomic locations included in the low-level annotation for the SingleCellExperiment  
location <- rowRanges(sce.416b)
is.mito <- any(seqnames(location) == "MT")

df <- perCellQCMetrics(sce.416b, subsets = list(Mito = is.mito))
```


Alternatively, users may prefer to use the `addPerCellQC()` function. This automatically appends the per-cell QC statistics to the `colData` of the `SingleCellExperiment` object, allowing us to retain all relevant information in a single object for later manipulation.  
```{r}
sce.416b <- addPerCellQC(sce.416b, subsets = list(Mito = is.mito))
colnames(colData(sce.416b))
```


A key assumption here is that the QC metrics are independent of the biological state of each cell. Poor values (e.g., low library sizes, high mitochondrial proportions) are presumed to be driven by technical factors rather than biological processes, meaning that the subsequent removal of cells will not misrepresent the biology in downstream analyses. Major violations of this assumption would potentially result in the loss of cell types that have, say, systematically low RNA content or high numbers of mitochondria.  


## Identifying low-quanlity cells  

### with fixed thresholds  
The simplest approach to identifying low-quality cells is to apply thresholds on the QC metrics. For example, we might consider cells to be low quality if they have library sizes below 100,000 reads; express fewer than 5,000 genes; have spike-in proportions above 10%; or have mitochondrial proportions above 10%.  
```{r}
qc.lib <- df$sum < 1e5
qc.nexprs <- df$detected < 5e3
qc.spike <- df$altexps_ERCC_percent > 10
qc.mito <- df$subsets_Mito_percent > 10
discard <- qc.lib | qc.nexprs | qc.spike | qc.mito

# summarize the number of cells removed for each reason
DataFrame(
    LibSize = sum(qc.lib), 
    NExprs = sum(qc.nexprs), 
    SpikeProp = sum(qc.spike), 
    MitoProp = sum(qc.mito), 
    Total = sum(discard)
)
```

While simple, this strategy requires considerable experience to determine appropriate thresholds for each experimental protocol and biological system. Thresholds for read count-based data are simply not applicable for UMI-based data, and vice versa. Differences in mitochondrial activity or total RNA content require constant adjustment of the mitochondrial and spike-in thresholds, respectively, for different biological systems. Indeed, even with the same protocol and system, the appropriate threshold can vary from run to run due to the vagaries of cDNA capture efficiency and sequencing depth per cell.  

### with adaptive thresholds  
#### identifying outliers  
To obtain an adaptive threshold, we assume that most of the dataset consists of high-quality cells. We then identify cells that are outliers for the various QC metrics, based on the median absolute deviation (MAD) from the median value of each metric across all cells. Specifically, a value is considered an outlier if it is more than 3 MADs from the median in the “problematic” direction. This is loosely motivated by the fact that such a filter will retain 99% of non-outlier values that follow a normal distribution.  

For the 416B data, we identify cells with log-transformed library sizes that are more than 3 MADs below the median. A log-transformation is used to improve resolution at small values when type="lower". In particular, it guarantees that the threshold is not a negative value, which would be meaningless for quality control on these metrics. Moreover, these metrics can occasionally exhibit a heavy right tail, and the log-transformation makes the distribution seem more normal to justify the 99% rationale mentioned above.  

```{r}
qc.lib2 <- isOutlier(df$sum, log = TRUE, type = "lower")
qc.nexprs2 <- isOutlier(df$detected, log = TRUE, type = "lower")
```

`isOutlier()` will also return the exact filter thresholds for each metric in the attributes of the output vector. These is useful for checking whether the automatically selected thresholds are appropriate.  
```{r}
attr(qc.lib2, "thresholds")
```

```{r}
attr(qc.nexprs2, "thresholds")
```

We identify outliers for the proportion-based metrics in a similar manner. Here, no transformation is performed as we are aiming to identify and remove large outliers that should already be clearly distinguishable from zero.  
```{r}
qc.spike2 <- isOutlier(df$altexps_ERCC_percent, type = "higher")
attr(qc.spike2, "thresholds")
```

```{r}
qc.mito2 <- isOutlier(df$subsets_Mito_percent, type="higher")
attr(qc.mito2, "thresholds")
```

```{r}
discard2 <- qc.lib2 | qc.nexprs2 | qc.spike2 | qc.mito2

# Summarize the number of cells removed for each reason.
DataFrame(LibSize=sum(qc.lib2), NExprs=sum(qc.nexprs2),
    SpikeProp=sum(qc.spike2), MitoProp=sum(qc.mito2), Total=sum(discard2))
```

Alternatively, this entire process can be done in a single step using the `quickPerCellQC()` function. This is a wrapper that simply calls `isOutlier()` with the settings described above.  
```{r}
reasons <- quickPerCellQC(df, percent_subsets=c("subsets_Mito_percent",
    "altexps_ERCC_percent"))
colSums(as.matrix(reasons))
```

With this strategy, the thresholds adapt to both the location and spread of the distribution of values for a given metric. This allows the QC procedure to adjust to changes in sequencing depth, cDNA capture efficiency, mitochondrial content, etc. without requiring any user intervention or prior experience. However, it does require some implicit assumptions that are discussed below in more detail.  


Outlier detection assumes that most cells are of acceptable quality. This is usually reasonable and can be experimentally supported in some situations by visually checking that the cells are intact, e.g., on the microwell plate. If most cells are of (unacceptably) low quality, the adaptive thresholds will obviously fail as they cannot remove the majority of cells. Of course, what is acceptable or not is in the eye of the beholder - neurons, for example, are notoriously difficult to dissociate, and we would often retain cells in a neuronal scRNA-seq dataset with QC metrics that would be unacceptable in a more amenable system like embryonic stem cells.  


Another assumption discussed earlier is that the QC metrics are independent of the biological state of each cell. This assumption is most likely to be violated in highly heterogeneous cell populations where cell types that naturally have less RNA or more mitochondria are more likely to be considered outliers and removed, even if they are of high quality. The use of the MAD mitigates this problem to some extent by accounting for biological variability in the QC metrics. A heterogeneous population should have higher variability in the metrics among high-quality cells, increasing the MAD and reducing the chance of incorrectly removing particular cell types (at the cost of reducing power to remove low-quality cells).  

#### Assumptions of outlier detection  
```{r}
sce.grun <- GrunPancreasData()
```










