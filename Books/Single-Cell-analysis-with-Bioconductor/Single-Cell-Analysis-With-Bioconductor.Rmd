---
title: "Single-Cell Analysis With Bioconductor"
author: "yincy"
date: "3/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(SingleCellExperiment)
library(scater)
library(scran)
library(uwot)
library(Rtsne)
library(scRNAseq)
```

[From this web site](https://osca.bioconductor.org/)  

# Chapter 4 Data Structure  
```{r}
knitr::include_graphics(path = "figures/SingleCellExperiment-strucutre.PNG")
```


## 4.2 Storing primary experimental data  
```{r}
count_matrix <- data.frame(
    cell_1 = rpois(n = 10, 10), 
    cell_2 = rpois(n = 10, 10), 
    cell_3 = rpois(n = 10, 30)
)

rownames(count_matrix) <- paste("gene_", 1:10, sep = "")
count_matrix <- as.matrix(count_matrix)
```

The SingleCellExperiment class inherits from the SummarizedExperiment class with several additional slots.  

- `reducedDims`: A SimpleList containing matrices of cell coordinates.  
- `int_elementMetadata`: A DataFrame containing internal row metadata (for each genomic features).  
- `int_colData`: A DataFrame containing internal column metadata (for each cell).  
- `int_metadata`: A list containing internal experiment metadata.  

The additional reducedDims slot allows storage of results from multiple dimensionality reduction methods, e.g., PCA or t-SNE. Each element of the SimpleList should be a matrix of coordinates for all cells from one reduction method. The number of rows of each matrix should be equal to the number of cells in the SingleCellExperiment object.  

```{r}
sce <- SingleCellExperiment(
    assays = list(counts = count_matrix)
)
```

```{r}
sce
```

```{r}
assay(sce, 1)
# or assay(sce, "counts")
```

```{r}
sce <- logNormCounts(x = sce, exprs_values = "counts")
```

```{r}
assays(sce)
```

```{r}
assay(sce, "logcounts")
```


```{r}
logcounts(sce)
```

```{r}
counts_100 <- counts(sce) + 100
assay(sce, "counts_100") <- counts_100

assays(sce)
assay(sce, "counts_100")
```

```{r}
cell_metadata <- data.frame(batch = c(1, 1, 2))
rownames(cell_metadata) <- paste("cell_", 1:3, sep = "")
```


```{r}
sce <- SingleCellExperiment(assays = list(counts = count_matrix), 
                            colData = cell_metadata)
```

```{r}
colData(sce)
```

```{r}
sce$batch
```

```{r}
sce <- scater::addPerCellQC(sce)
colData(sce)
```

```{r}
sce$more_stuff <- runif(ncol(sce))
colData(sce)
```

A common operation with colData is to use its values for subsetting.  
```{r}
sce[, sce$batch == 1]
```


```{r}
rowRanges(sce) # to store GRanges or GRangesList object
```

```{r}
rowqc <- data.frame(
    mean = apply(assay(sce, "counts"), 1, mean, na.rm = T), 
    detected = apply(assay(sce, "counts"), 1, function(x)mean(x > 0) * 100)
)

rowqc

# or using this way 
sce <- scater::addPerFeatureQC(sce)
```

```{r}
rowData(sce) <- rowqc
```

```{r}
sce
```

```{r}
sce[c("gene_1", "gene_3"), ]
```

```{r}
sce[c(1, 3),] # same as above
```


Some other meta data do not directly related to cells or samples can be stored in `metadata` slot.  
```{r}
my_genes <- c("gene_1", "gene_5")
metadata(sce) <- list(favorite_genes = my_genes)
metadata(sce)
```

```{r}
your_geen <- c("gene_4", "gene_8")
metadata(sce)$your_gene <- your_geen

metadata(sce)
```

The `reducedDims` slot is specially designed to store reduced dimensionality representations of the primary data obtained by methods such as PCA and t-SNE (see Chapter 9 for more details). This slot contains a list of numeric matrices of low-reduced representations of the primary data, where the rows represent the columns of the primary data (i.e., cells), and columns represent the dimensions.  

```{r}
sce <- logNormCounts(sce)
sce <- runPCA(sce)

x <- reducedDim(sce, "PCA")
attr(x, "percentVar")
```

```{r}
sce <- runTSNE(sce, perplexity = 0.1)

reducedDim(sce, "TSNE")
```

```{r}
u <- uwot::umap(t(logcounts(sce)), n_neighbors = 2)
reducedDim(sce, "UMAP") <- u
reducedDims(sce)
```

```{r}
reducedDim(sce, "UMAP")
```


```{r}
sce <- scran::computeSumFactors(sce)
```


# Chapter 5 Overview  
```{r, fig.cap="scRNA-seq analysis workflow"}
knitr::include_graphics(path = "figures/SingeCellExperiment-analysis-flow.PNG")
```

## 5.2 Experimental Design  
**Sequencing technology**:  

- Droplet-based: 10X Genomics, inDrop, Drop-seq  

- Plate-based with unique molecular identifers (UMIs): CEL-seq, MARS-seq  

- Plate-base with reads: Smart-seq2  

- Other: sci-RNA-seq, Seq-Well  


## Obtaining a count matrix  
- For 10X Genomics data, the CellRanger software suite provides a custom pipeline to obtain a count matrix. This uses STAR to align reads to the reference genome and then counts the number of unique UMIs mapped to each gene.  

- Pseudo-alignment methods such as alevin can be used to obtain a count matrix from the same data with greater efficiency. This avoids the need for explicit alignment, which reduces the compute time and memory usage.  

- For other highly multiplexed protocols, the `scPipe` package provides a more general pipeline for processing scRNA-seq data. This uses the `Rsubread` aligner to align reads and then counts UMIs per gene.  

- For CEL-seq or CEL-seq2 data, the `scruff` package provides a dedicated pipeline for quantification.  

- For read-based protocols, we can generally re-use the same pipelines for processing bulk RNA-seq data.  

- For any data involving spike-in transcripts, the spike-in sequences should be included as part of the reference genome during alignment and quantification.  

After quantification, we import the count matrix into R and create a SingleCellExperiment object. This can be done with base methods (e.g., read.table()) followed by applying the SingleCellExperiment() constructor. Alternatively, for specific file formats, we can use dedicated methods from the `DropletUtils` (for 10X data) or `tximport`/`tximeta` packages (for pseudo-alignment methods).  


## Data processing and downstream analysis  
In the simplest case, the workflow has the following form:  

1. **We compute quality control metrics to remove low-quality cells that would interfere with downstream analyses**. These cells may have been damaged during processing or may not have been fully captured by the sequencing protocol. Common metrics includes the total counts per cell, the proportion of spike-in or mitochondrial reads and the number of detected features.  

2. **We convert the counts into normalized expression values to eliminate cell-specific biases (e.g., in capture efficiency)**. This allows us to perform explicit comparisons across cells in downstream steps like clustering. We also apply a transformation, typically log, to adjust for the mean-variance relationship.  

3. **We perform feature selection to pick a subset of interesting features for downstream analysis**. This is done by modelling the variance across cells for each gene and retaining genes that are highly variable. The aim is to reduce computational overhead and noise from uninteresting genes.  

4. **We apply dimensionality reduction to compact the data and further reduce noise**. Principal components analysis is typically used to obtain an initial low-rank representation for more computational work, followed by more aggressive methods like t-stochastic neighbor embedding for visualization purposes.  

5. **We cluster cells into groups according to similarities in their (normalized) expression profiles**. This aims to obtain groupings that serve as empirical proxies for distinct biological states. We typically interpret these groupings by identifying differentially expressed marker genes between clusters.  


# Chapter 6 Quality Control  
Low-quality libraries in scRNA-seq data can arise from a variety of sources such as cell damage during dissociation or failure in library preparation (e.g., inefficient reverse transcription or PCR amplification). These usually manifest as “cells” with low total counts, few expressed genes and high mitochondrial or spike-in proportions. These low-quality libraries are problematic as they can contribute to misleading results in downstream analyses.  

- They form their own distinct cluster(s), complicating interpretation of the results. This is most obviously driven by increased mitochondrial proportions or enrichment for nuclear RNAs after cell damage. In the worst case, low-quality libraries generated from different cell types can cluster together based on similarities in the damage-induced expression profiles, creating artificial intermediate states or trajectories between otherwise distinct subpopulations. Additionally, very small libraries can form their own clusters due to shifts in the mean upon transformation.  

- They distort the characterization of population heterogeneity during variance estimation or principal components analysis. The first few principal components will capture differences in quality rather than biology, reducing the effectiveness of dimensionality reduction. Similarly, genes with the largest variances will be driven by differences between low- and high-quality cells. The most obvious example involves low-quality libraries with very low counts where scaling normalization inflates the apparent variance of genes that happen to have a non-zero count in those libraries.  

- They contain genes that appear to be strongly “upregulated” due to aggressive scaling to normalize for small library sizes. This is most problematic for contaminating transcripts (e.g., from the ambient solution) that are present in all libraries at low but constant levels. Increased scaling in low-quality libraries transforms small counts for these transcripts in large normalized expression values, resulting in apparent upregulation compared to other cells. This can be misleading as the affected genes are often biologically sensible but are actually expressed in another subpopulation.  

To avoid - or at least mitigate - these problems, we need to remove these cells at the start of the analysis. This step is commonly referred to as quality control (QC) on the cells.  

```{r}
sce.416b <- LunSpikeInData(which = "416b")
sce.416b
```


## Choice of QC metrics  
We use several common QC metrics to identify low-quality cells based on their expression profiles. These metrics are described below in terms of reads for SMART-seq2 data, but the same definitions apply to UMI data generated by other technologies like MARS-seq and droplet-based protocols.  

- The library size is defined as the total sum of counts across all relevant features for each cell. Here, we will consider the relevant features to be the endogenous genes. Cells with small library sizes are of low quality as the RNA has been lost at some point during library preparation, either due to cell lysis or inefficient cDNA capture and amplification.  

- The number of expressed features in each cell is defined as the number of endogenous genes with non-zero counts for that cell. Any cell with very few expressed genes is likely to be of poor quality as the diverse transcript population has not been successfully captured.  

- The proportion of reads mapped to spike-in transcripts is calculated relative to the total count across all features (including spike-ins) for each cell. As the same amount of spike-in RNA should have been added to each cell, any enrichment in spike-in counts is symptomatic of loss of endogenous RNA. Thus, high proportions are indicative of poor-quality cells where endogenous RNA has been lost due to, e.g., partial cell lysis or RNA degradation during dissociation.  

- In the absence of spike-in transcripts, the proportion of reads mapped to genes in the mitochondrial genome can be used. High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), presumably because of loss of cytoplasmic RNA from perforated cells. The reasoning is that, in the presence of modest damage, the holes in the cell membrane permit efflux of individual transcript molecules but are too small to allow mitochondria to escape, leading to a relative enrichment of mitochondrial transcripts.  

For each cell, we calculate these QC metrics using the perCellQCMetrics() function from the `scater` package (McCarthy et al. 2017). The `sum` column contains the total count for each cell, the `detected` column contains the number of detected genes, `subsets_Mito_percent` contains the percentage of reads mapped to mitochondrial transcripts (based on Ensembl annotation) and `altexps_ERCC_percent` contains the percentage of reads mapped to ERCC transcripts.  

```{r}
# retrieving the mitochondrial transcripts using genomic locations included in the low-level annotation for the SingleCellExperiment  
location <- rowRanges(sce.416b)
is.mito <- any(seqnames(location) == "MT")
```

A key assumption here is that the QC metrics are independent of the biological state of each cell. Poor values (e.g., low library sizes, high mitochondrial proportions) are presumed to be driven by technical factors rather than biological processes, meaning that the subsequent removal of cells will not misrepresent the biology in downstream analyses. Major violations of this assumption would potentially result in the loss of cell types that have, say, systematically low RNA content or high numbers of mitochondria.  


## Identifying low-quanlity cells  
The simplest approach to identifying low-quality cells is to apply thresholds on the QC metrics. For example, we might consider cells to be low quality if they have library sizes below 100,000 reads; express fewer than 5,000 genes; have spike-in proportions above 10%; or have mitochondrial proportions above 10%.  

```{r}
qc.lib <- df$sum < 1e5
```













