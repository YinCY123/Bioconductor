---
title: "dbplyr"
author: "yincy"
date: "3/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction to dbplyr  
As well as working with local in-memory data stored in data frames, dplyr also works with remote on-disk data stored in databases. This is particularly useful in two scenarios:  

- Your data is already in a database.  
- You have so much data that it does not all fit into memory simultaneously and you need to use some external storage engine.  

This vignette focuses on the first scenario because it’s the most common. If you’re using R to do data analysis inside a company, most of the data you need probably already lives in a database (it’s just a matter of figuring out which one!). However, you will learn how to load data in to a local database in order to demonstrate dplyr’s database tools. At the end, I’ll also give you a few pointers if you do need to set up your own database.  

### Getting started  
```{r}
library(dbplyr)
library(DBI)
```

You’ll also need to install a DBI backend package. The DBI package provides a common interface that allows dplyr to work with many different databases using the same code. DBI is automatically installed with dbplyr, but you need to install a specific backend for the database that you want to connect to.  

Five commonly used backends are:  

- `RMariaDB` connects to MySQL and MariaDB  
- `RPostgres` connects to Postgres and Redshift  
- `RSQLite` embeds a SQLite database.  
- `odbc` connects to many commercial databases via the open database connectivity protocol.  
- `bigrquery` connects to Google's BigQuery.  

If the database you need to connect to is not listed here, you’ll need to do some investigation (i.e. googling) yourself.  

SQLite is a great way to get started with databases because it’s completely embedded inside an R package. Unlike most other systems, you don’t need to setup a separate database server. SQLite is great for demos, but is surprisingly powerful, and with a little practice you can use it to easily work with many gigabytes of data.  


### Connecting to the database  
To work with a database in dplyr, you must first connect to it, using `DBI::dbConnect()`. We’re not going to go into the details of the DBI package here, but it’s the foundation upon which dbplyr is built. You’ll need to learn more about if you need to do things to the database that are beyond the scope of dplyr.  
```{r}
con <- DBI::dbConnect(drv = RSQLite::SQLite(), dbname = ":memory:")
```

The arguments to DBI::dbConnect() vary from database to database, but the first argument is always the database backend. It’s `RSQLite::SQLite()` for RSQLite, `RMariaDB::MariaDB()` for RMariaDB, `RPostgres::Postgres()` for RPostgres, `odbc::odbc()` for odbc, and `bigrquery::bigquery()` for BigQuery. SQLite only needs one other argument: the path to the database. Here we use the special string `":memory:"` which causes SQLite to make a temporary in-memory database.  

Most existing databases don’t live in a file, but instead live on another server. That means in real-life that your code will look more like this:  
```
con <- DBI::dbConnect(drv = RMariaDB::MariaDB(), 
                      host = "database,rstudio.com", 
                      user = "hadley", 
                      password = rstudioapi::askForPassword(prompt = "Please enter your password"))
```

```{r}
copy_to(con, nycflights13::flights, "flights", 
        indexs = list(
            c("year", "month", "day"), 
            "carrier",
            "tailnum", 
            "dest"
        ), 
        overwrite = T)
```

As you can see, the copy_to() operation has an additional argument that allows you to supply indexes for the table. Here we set up indexes that will allow us to quickly process the data by day, carrier, plane, and destination. Creating the right indices is key to good database performance, but is unfortunately beyond the scope of this article.  

```{r}
flights <- tbl(con, "flights")
```


### Generating queries  
To interact with a database you usually use SQL, the Structured Query Language. SQL is over 40 years old, and is used by pretty much every database in existence. The goal of dbplyr is to automatically generate SQL for you so that you’re not forced to use it. However, SQL is a very large language and dbplyr doesn’t do everything. It focusses on SELECT statements, the SQL you write most often as an analyst.  

Most of the time you don’t need to know anything about SQL, and you can continue to use the dplyr verbs that you’re already familiar with:  
```{r}
flights %>% 
    select(year:day, dep_delay, arr_delay)
```


```{r}
flights %>% 
    filter(dep_delay > 240)
```

```{r}
flights %>% 
    group_by(dest) %>% 
    summarise(delay = mean(dep_time))
```


However, in the long-run, I highly recommend you at least learn the basics of SQL. It’s a valuable skill for any data scientist, and it will help you debug problems if you run into problems with dplyr’s automatic translation. If you’re completely new to SQL you might start with this [codeacademy tutorial](https://www.codecademy.com/learn/learn-sql). If you have some familiarity with SQL and you’d like to learn more, I found [how indexes work in SQLite](http://www.sqlite.org/queryplanner.html) and [10 easy steps to a complete understanding of SQL](http://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql) to be particularly helpful.  

The most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database on the remote server, not in R on your local machine. When working with databases, dplyr tries to be as lazy as possible:  

- It never pulls data into R unless you explicitly ask for it.  
- It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.  

For example, take the following code:  
```{r}
tailnum_delay_db <- flights %>% 
    group_by(tailnum) %>% 
    summarise(delay =mean(arr_delay), 
              n = n()) %>% 
    arrange(-delay) %>% 
    filter(n > 100)
```

Surprisingly, this sequence of operations never touches the database. It’s not until you ask for the data (e.g. by printing tailnum_delay) that dplyr generates the SQL and requests the results from the database. Even then it tries to do as little work as possible and only pulls down a few rows.  

Behind the scenes, dplyr is translating your R code into SQL. You can see the SQL it’s generating with show_query():  
```{r}
tailnum_delay_db %>% show_query()
```

If you’re familiar with SQL, this probably isn’t exactly what you’d write by hand, but it does the job. You can learn more about the SQL translation in vignette("translation-verb") and vignette("translation-function").  

Typically, you’ll iterate a few times before you figure out what data you need from the database. Once you’ve figured it out, use collect() to pull all the data down into a local tibble:  

```{r}
tailnum_delay <- tailnum_delay_db %>% collect()
```

`collect()` requires that database does some work, so it may take a long time to complete. Otherwise, dplyr tries to prevent you from accidentally performing expensive query operations:  

- Because there's generally no way to determine how many rows a query will return unless you actually run it, `nrow()` is always NA.  
- Because you can't find the last few rows without executing the whole query, you can't use `tail()`.  
```{r}
nrow(tailnum_delay_db)
tail(tailnum_delay_db)
```

You can also ask the database how it plans to execute the query with explain(). The output is database dependent, and can be esoteric, but learning a bit about it can be very useful because it helps you understand if the database can execute the query efficiently, or if you need to create new indices.  










































